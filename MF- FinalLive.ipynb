{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247a6341",
   "metadata": {},
   "source": [
    "### *Live Feed Hard Coded SpliceVid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f5ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Create RepVids directory in \"Deep Dive AI Summer 2025\" folder\n",
    "save_dir = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"Deep Dive AI Summer 2025\", \"RepVids\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize variables\n",
    "frame_buffer = []\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "buffer_size = 90  # Number of frames to keep in buffer\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def calc_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    return 360 - angle if angle > 180 else angle\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "counter = 0\n",
    "stage = None\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,\n",
    "                  min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Always add current frame to buffer\n",
    "        frame_buffer.append(image.copy())\n",
    "        \"\"\"\n",
    "        if len(frame_buffer) > buffer_size:\n",
    "            frame_buffer.pop(0)  # Keep only last 90 frames\n",
    "        \"\"\"\n",
    "\n",
    "        label = \"None\"\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            h, w, _ = image.shape\n",
    "\n",
    "            # Get right side landmarks\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                        landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                   landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "            knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                    landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "            ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "\n",
    "            # Visibility check\n",
    "            visible = all(landmarks[i].visibility > 0.7 for i in [\n",
    "                mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_ANKLE.value\n",
    "            ])\n",
    "            \n",
    "            # Horizontal filter\n",
    "            locs = np.array([\n",
    "                shoulder[1] * h, elbow[1] * h, wrist[1] * h,\n",
    "                hip[1] * h, knee[1] * h, ankle[1] * h\n",
    "            ])\n",
    "            data_range = np.max(locs) - np.min(locs)\n",
    "            if data_range > 0.5 * h:\n",
    "                visible = False\n",
    "            downUp = False\n",
    "            \n",
    "\n",
    "            if visible:\n",
    "                elbow_angle = calc_angle(shoulder, elbow, wrist)\n",
    "                back_angle = calc_angle(shoulder, hip, knee)\n",
    "                knee_angle = calc_angle(hip, knee, ankle)\n",
    "\n",
    "                # Pushup logic\n",
    "                percent = np.interp(elbow_angle, [90, 160], [0, 100])\n",
    "                \n",
    "                # Store previous counter to detect changes\n",
    "                prev_counter = counter\n",
    "\n",
    "                # Track full cycle: up -> down -> up\n",
    "                if elbow_angle > 165:\n",
    "                    if stage == \"down\":\n",
    "                        stage = \"up\"\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        stage = \"up\"\n",
    "                elif elbow_angle < 95 and stage == \"up\":\n",
    "                    stage = \"down\"\n",
    "\n",
    "                # SAVE VIDEO EVERY TIME COUNTER INCREASES\n",
    "                if counter > prev_counter:\n",
    "                    filename = os.path.join(save_dir, f'rep_{counter:03d}.mp4')\n",
    "                    \n",
    "                    if len(frame_buffer) > 0:\n",
    "                        out = cv2.VideoWriter(filename, fourcc, 20.0, (image.shape[1], image.shape[0]))\n",
    "                        \n",
    "                        for f in frame_buffer:\n",
    "                            out.write(f)\n",
    "                        out.release()\n",
    "                        \n",
    "                        print(f\"Saved rep {counter} to: {filename} ({len(frame_buffer)} frames)\")\n",
    "\n",
    "                        frame_buffer = []  # Reset buffer for next rep\n",
    "                    else:\n",
    "                        print(f\"No frames to save for rep {counter}\")\n",
    "\n",
    "                label = \"Pushup\"\n",
    "\n",
    "                # Draw angle + percent\n",
    "                cv2.putText(image, f'Elbow: {int(elbow_angle)}', (10, 100),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                cv2.putText(image, f'{int(percent)}%', (10, 140),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                label = \"None\"\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Draw buffer status\n",
    "        cv2.putText(image, f'Buffer: {len(frame_buffer)}/{buffer_size}', (10, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "\n",
    "        # Draw pose\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Label\n",
    "        cv2.putText(image, f'Form: {label}', (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 2)\n",
    "\n",
    "        # Rep counter\n",
    "        cv2.rectangle(image, (image.shape[1] - 160, 0), (image.shape[1], 80), (0, 0, 0), -1)\n",
    "        cv2.putText(image, 'REPS', (image.shape[1] - 150, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        cv2.putText(image, str(counter), (image.shape[1] - 140, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 4)\n",
    "\n",
    "        cv2.imshow('Push-Up Detector', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced first rep handling with stage-based frame collection\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "# Create RepVids directory in \"Deep Dive AI Summer 2025\" folder\n",
    "save_dir = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"Deep Dive AI Summer 2025\", \"RepVids\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize variables\n",
    "frame_buffer = []\n",
    "first_rep_buffer = []  # Special buffer for first rep\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "buffer_size = 90\n",
    "first_stage_up_detected = False  # Track when stage first becomes \"up\"\n",
    "is_first_rep = True  # Track if we're on the first rep\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def calc_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    return 360 - angle if angle > 180 else angle\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "counter = 0\n",
    "stage = None\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,\n",
    "                  min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Always add current frame to main buffer for non-first reps\n",
    "        if not is_first_rep:\n",
    "            frame_buffer.append(image.copy())\n",
    "\n",
    "        label = \"None\"\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            h, w, _ = image.shape\n",
    "\n",
    "            # Get right side landmarks\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                        landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                   landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "            knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                    landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "            ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "\n",
    "            # Visibility check\n",
    "            visible = all(landmarks[i].visibility > 0.7 for i in [\n",
    "                mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_ANKLE.value\n",
    "            ])\n",
    "            \n",
    "            # Horizontal filter\n",
    "            locs = np.array([\n",
    "                shoulder[1] * h, elbow[1] * h, wrist[1] * h,\n",
    "                hip[1] * h, knee[1] * h, ankle[1] * h\n",
    "            ])\n",
    "            data_range = np.max(locs) - np.min(locs)\n",
    "            if data_range > 0.5 * h:\n",
    "                visible = False\n",
    "\n",
    "            if visible:\n",
    "                elbow_angle = calc_angle(shoulder, elbow, wrist)\n",
    "                back_angle = calc_angle(shoulder, hip, knee)\n",
    "                knee_angle = calc_angle(hip, knee, ankle)\n",
    "\n",
    "                # Pushup logic\n",
    "                percent = np.interp(elbow_angle, [90, 160], [0, 100])\n",
    "                \n",
    "                # Store previous counter and stage to detect changes\n",
    "                prev_counter = counter\n",
    "                prev_stage = stage\n",
    "\n",
    "                # Track full cycle: up -> down -> up\n",
    "                if elbow_angle > 165:\n",
    "                    if stage == \"down\":\n",
    "                        stage = \"up\"\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        stage = \"up\"\n",
    "                elif elbow_angle < 95 and stage == \"up\":\n",
    "                    stage = \"down\"\n",
    "\n",
    "                # FIRST REP SPECIAL HANDLING\n",
    "                if is_first_rep:\n",
    "                    # Start collecting frames when stage first becomes \"up\"\n",
    "                    if stage == \"up\" and not first_stage_up_detected:\n",
    "                        first_stage_up_detected = True\n",
    "                        first_rep_buffer = [image.copy()]  # Start with current frame\n",
    "                        print(\"ðŸ”´ First rep: Started collecting frames (stage = 'up')\")\n",
    "                    \n",
    "                    # Continue collecting frames if we've started\n",
    "                    elif first_stage_up_detected:\n",
    "                        first_rep_buffer.append(image.copy())\n",
    "                    \n",
    "                    # Save first rep when counter increases from 0 to 1\n",
    "                    if counter > prev_counter and counter == 1:\n",
    "                        filename = os.path.join(save_dir, f'rep_{counter:03d}.mp4')\n",
    "                        \n",
    "                        if len(first_rep_buffer) > 0:\n",
    "                            out = cv2.VideoWriter(filename, fourcc, 20.0, (image.shape[1], image.shape[0]))\n",
    "                            \n",
    "                            for f in first_rep_buffer:\n",
    "                                out.write(f)\n",
    "                            out.release()\n",
    "                            \n",
    "                            print(f\"âœ… Saved FIRST rep {counter} to: {filename} ({len(first_rep_buffer)} frames)\")\n",
    "                            print(f\"   Frames collected from first 'up' stage to rep completion\")\n",
    "                            \n",
    "                            # Reset first rep variables\n",
    "                            first_rep_buffer = []\n",
    "                            is_first_rep = False\n",
    "                            print(\"ðŸ”„ Switching to normal rep recording mode\")\n",
    "                        else:\n",
    "                            print(f\"âŒ No frames to save for first rep {counter}\")\n",
    "\n",
    "                # NORMAL REP HANDLING (for reps 2+)\n",
    "                elif counter > prev_counter:\n",
    "                    filename = os.path.join(save_dir, f'rep_{counter:03d}.mp4')\n",
    "                    \n",
    "                    if len(frame_buffer) > 0:\n",
    "                        out = cv2.VideoWriter(filename, fourcc, 20.0, (image.shape[1], image.shape[0]))\n",
    "                        \n",
    "                        for f in frame_buffer:\n",
    "                            out.write(f)\n",
    "                        out.release()\n",
    "                        \n",
    "                        print(f\"âœ… Saved rep {counter} to: {filename} ({len(frame_buffer)} frames)\")\n",
    "\n",
    "                        frame_buffer = []  # Reset buffer for next rep\n",
    "                    else:\n",
    "                        print(f\"âŒ No frames to save for rep {counter}\")\n",
    "\n",
    "                label = \"Pushup\"\n",
    "\n",
    "                # Draw angle + percent\n",
    "                cv2.putText(image, f'Elbow: {int(elbow_angle)}', (10, 100),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                cv2.putText(image, f'{int(percent)}%', (10, 140),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                label = \"None\"\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Draw buffer status with first rep indicator\n",
    "        if is_first_rep:\n",
    "            if first_stage_up_detected:\n",
    "                cv2.putText(image, f'First Rep Buffer: {len(first_rep_buffer)}', (10, 70),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(image, 'Waiting for first \"up\" stage...', (10, 70),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "        else:\n",
    "            cv2.putText(image, f'Buffer: {len(frame_buffer)}/{buffer_size}', (10, 70),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "\n",
    "        # Draw pose\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Label with first rep indicator\n",
    "        rep_status = \" (FIRST REP)\" if is_first_rep else \"\"\n",
    "        cv2.putText(image, f'Form: {label}{rep_status}', (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 2)\n",
    "\n",
    "        # Rep counter\n",
    "        cv2.rectangle(image, (image.shape[1] - 160, 0), (image.shape[1], 80), (0, 0, 0), -1)\n",
    "        cv2.putText(image, 'REPS', (image.shape[1] - 150, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        cv2.putText(image, str(counter), (image.shape[1] - 140, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 4)\n",
    "\n",
    "        cv2.imshow('Push-Up Detector', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8e782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error during execution: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Push-up Detection System with MediaPipe\n",
    "\n",
    "This module provides a comprehensive push-up detection and recording system\n",
    "using MediaPipe pose estimation and OpenCV for video processing.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DetectionConfig:\n",
    "    \"\"\"Configuration parameters for push-up detection.\"\"\"\n",
    "    \n",
    "    # MediaPipe confidence thresholds\n",
    "    min_detection_confidence: float = 0.5\n",
    "    min_tracking_confidence: float = 0.5\n",
    "    \n",
    "    # Angle thresholds for push-up detection\n",
    "    elbow_angle_up_threshold: float = 165.0\n",
    "    elbow_angle_down_threshold: float = 95.0\n",
    "    \n",
    "    # Visibility and filtering thresholds\n",
    "    min_visibility_threshold: float = 0.7\n",
    "    horizontal_filter_ratio: float = 0.5\n",
    "    \n",
    "    # Video recording parameters\n",
    "    fps: float = 20.0\n",
    "    fourcc: str = 'mp4v'\n",
    "    buffer_size: int = 90\n",
    "    \n",
    "    # File paths\n",
    "    save_directory: str = \"RepVids\"\n",
    "    \n",
    "    # UI parameters\n",
    "    font_scale: float = 0.7\n",
    "    font_thickness: int = 2\n",
    "\n",
    "\n",
    "class LandmarkExtractor:\n",
    "    \"\"\"Utility class for extracting and processing pose landmarks.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_landmarks(results, mp_pose) -> Optional[Dict[str, List[float]]]:\n",
    "        \"\"\"Extract relevant landmarks from MediaPipe results.\"\"\"\n",
    "        if not results.pose_landmarks:\n",
    "            return None\n",
    "            \n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        return {\n",
    "            'shoulder': [\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y\n",
    "            ],\n",
    "            'elbow': [\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y\n",
    "            ],\n",
    "            'wrist': [\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y\n",
    "            ],\n",
    "            'hip': [\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y\n",
    "            ],\n",
    "            'knee': [\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y\n",
    "            ],\n",
    "            'ankle': [\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_visibility(results, mp_pose, threshold: float = 0.7) -> bool:\n",
    "        \"\"\"Check if all required landmarks are visible enough.\"\"\"\n",
    "        if not results.pose_landmarks:\n",
    "            return False\n",
    "            \n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        required_landmarks = [\n",
    "            mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_ANKLE.value\n",
    "        ]\n",
    "        \n",
    "        return all(landmarks[i].visibility > threshold for i in required_landmarks)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_horizontal_filter(landmark_dict: Dict[str, List[float]], \n",
    "                              image_height: int, \n",
    "                              filter_ratio: float = 0.5) -> bool:\n",
    "        \"\"\"Apply horizontal filter to check if pose is suitable for detection.\"\"\"\n",
    "        y_coords = [\n",
    "            landmark_dict['shoulder'][1] * image_height,\n",
    "            landmark_dict['elbow'][1] * image_height,\n",
    "            landmark_dict['wrist'][1] * image_height,\n",
    "            landmark_dict['hip'][1] * image_height,\n",
    "            landmark_dict['knee'][1] * image_height,\n",
    "            landmark_dict['ankle'][1] * image_height\n",
    "        ]\n",
    "        \n",
    "        data_range = max(y_coords) - min(y_coords)\n",
    "        return data_range <= filter_ratio * image_height\n",
    "\n",
    "\n",
    "class AngleCalculator:\n",
    "    \"\"\"Utility class for calculating angles between body landmarks.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_angle(point_a: List[float], \n",
    "                       point_b: List[float], \n",
    "                       point_c: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate angle between three points.\n",
    "        \n",
    "        Args:\n",
    "            point_a: First point coordinates [x, y]\n",
    "            point_b: Vertex point coordinates [x, y]\n",
    "            point_c: Third point coordinates [x, y]\n",
    "            \n",
    "        Returns:\n",
    "            Angle in degrees\n",
    "        \"\"\"\n",
    "        a = np.array(point_a)\n",
    "        b = np.array(point_b)\n",
    "        c = np.array(point_c)\n",
    "        \n",
    "        radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "        angle = np.abs(radians * 180.0 / np.pi)\n",
    "        \n",
    "        return 360 - angle if angle > 180 else angle\n",
    "\n",
    "\n",
    "class VideoRecorder:\n",
    "    \"\"\"Handles video recording functionality for push-up reps.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DetectionConfig):\n",
    "        self.config = config\n",
    "        self.save_path = Path.home() / \"Desktop\" / \"Deep Dive AI Summer 2025\" / config.save_directory\n",
    "        self.save_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    @contextmanager\n",
    "    def create_video_writer(self, filename: str, frame_size: Tuple[int, int]):\n",
    "        \"\"\"Context manager for video writer.\"\"\"\n",
    "        fourcc = cv2.VideoWriter_fourcc(*self.config.fourcc)\n",
    "        filepath = self.save_path / filename\n",
    "        \n",
    "        writer = cv2.VideoWriter(\n",
    "            str(filepath), \n",
    "            fourcc, \n",
    "            self.config.fps, \n",
    "            frame_size\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            yield writer, filepath\n",
    "        finally:\n",
    "            writer.release()\n",
    "    \n",
    "    def save_rep_video(self, frames: List[np.ndarray], rep_number: int) -> bool:\n",
    "        \"\"\"Save a list of frames as a video file.\"\"\"\n",
    "        if not frames:\n",
    "            logging.warning(f\"No frames to save for rep {rep_number}\")\n",
    "            return False\n",
    "            \n",
    "        filename = f'rep_{rep_number:03d}.mp4'\n",
    "        frame_size = (frames[0].shape[1], frames[0].shape[0])\n",
    "        \n",
    "        try:\n",
    "            with self.create_video_writer(filename, frame_size) as (writer, filepath):\n",
    "                for frame in frames:\n",
    "                    writer.write(frame)\n",
    "                    \n",
    "            logging.info(f\"Saved rep {rep_number} to: {filepath} ({len(frames)} frames)\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save rep {rep_number}: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "class PushUpDetector:\n",
    "    \"\"\"Main push-up detection and tracking class.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DetectionConfig = None):\n",
    "        self.config = config or DetectionConfig()\n",
    "        self.video_recorder = VideoRecorder(self.config)\n",
    "        \n",
    "        # Detection state\n",
    "        self.counter = 0\n",
    "        self.stage = None\n",
    "        \n",
    "        # Frame buffers\n",
    "        self.frame_buffer = []\n",
    "        self.first_rep_buffer = []\n",
    "        \n",
    "        # First rep tracking\n",
    "        self.first_stage_up_detected = False\n",
    "        self.is_first_rep = True\n",
    "        \n",
    "        # MediaPipe setup\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        \n",
    "    def process_frame(self, frame: np.ndarray, pose_results) -> Tuple[np.ndarray, str]:\n",
    "        \"\"\"\n",
    "        Process a single frame for push-up detection.\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame from camera\n",
    "            pose_results: MediaPipe pose detection results\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (processed_frame, detection_label)\n",
    "        \"\"\"\n",
    "        processed_frame = frame.copy()\n",
    "        label = \"None\"\n",
    "        \n",
    "        # Extract landmarks\n",
    "        landmark_dict = LandmarkExtractor.extract_landmarks(pose_results, self.mp_pose)\n",
    "        if not landmark_dict:\n",
    "            return self._add_ui_elements(processed_frame, label)\n",
    "            \n",
    "        # Check visibility and apply filters\n",
    "        if not self._is_pose_valid(pose_results, landmark_dict, frame.shape[0]):\n",
    "            return self._add_ui_elements(processed_frame, label)\n",
    "            \n",
    "        # Calculate angles\n",
    "        elbow_angle = AngleCalculator.calculate_angle(\n",
    "            landmark_dict['shoulder'], \n",
    "            landmark_dict['elbow'], \n",
    "            landmark_dict['wrist']\n",
    "        )\n",
    "        \n",
    "        # Update detection state\n",
    "        prev_counter = self.counter\n",
    "        self._update_detection_state(elbow_angle)\n",
    "        \n",
    "        # Handle frame recording\n",
    "        self._handle_frame_recording(processed_frame, prev_counter)\n",
    "        \n",
    "        # Add angle and percentage display\n",
    "        percent = np.interp(elbow_angle, [90, 160], [0, 100])\n",
    "        self._add_angle_display(processed_frame, elbow_angle, percent)\n",
    "        \n",
    "        label = \"Pushup\"\n",
    "        \n",
    "        # Draw pose landmarks\n",
    "        self.mp_drawing.draw_landmarks(\n",
    "            processed_frame, \n",
    "            pose_results.pose_landmarks, \n",
    "            self.mp_pose.POSE_CONNECTIONS,\n",
    "            self.mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=4),\n",
    "            self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "        )\n",
    "        \n",
    "        return self._add_ui_elements(processed_frame, label)\n",
    "    \n",
    "    def _is_pose_valid(self, pose_results, landmark_dict: Dict, image_height: int) -> bool:\n",
    "        \"\"\"Check if the detected pose is valid for push-up detection.\"\"\"\n",
    "        # Check visibility\n",
    "        if not LandmarkExtractor.check_visibility(\n",
    "            pose_results, \n",
    "            self.mp_pose, \n",
    "            self.config.min_visibility_threshold\n",
    "        ):\n",
    "            return False\n",
    "            \n",
    "        # Apply horizontal filter\n",
    "        return LandmarkExtractor.apply_horizontal_filter(\n",
    "            landmark_dict, \n",
    "            image_height, \n",
    "            self.config.horizontal_filter_ratio\n",
    "        )\n",
    "    \n",
    "    def _update_detection_state(self, elbow_angle: float):\n",
    "        \"\"\"Update the push-up detection state based on elbow angle.\"\"\"\n",
    "        if elbow_angle > self.config.elbow_angle_up_threshold:\n",
    "            if self.stage == \"down\":\n",
    "                self.stage = \"up\"\n",
    "                self.counter += 1\n",
    "            else:\n",
    "                self.stage = \"up\"\n",
    "        elif elbow_angle < self.config.elbow_angle_down_threshold and self.stage == \"up\":\n",
    "            self.stage = \"down\"\n",
    "    \n",
    "    def _handle_frame_recording(self, frame: np.ndarray, prev_counter: int):\n",
    "        \"\"\"Handle frame recording for both first rep and subsequent reps.\"\"\"\n",
    "        if self.is_first_rep:\n",
    "            self._handle_first_rep_recording(frame, prev_counter)\n",
    "        else:\n",
    "            self._handle_normal_rep_recording(frame, prev_counter)\n",
    "    \n",
    "    def _handle_first_rep_recording(self, frame: np.ndarray, prev_counter: int):\n",
    "        \"\"\"Handle frame recording for the first repetition.\"\"\"\n",
    "        # Start collecting frames when stage first becomes \"up\"\n",
    "        if self.stage == \"up\" and not self.first_stage_up_detected:\n",
    "            self.first_stage_up_detected = True\n",
    "            self.first_rep_buffer = [frame.copy()]\n",
    "            logging.info(\"First rep: Started collecting frames (stage = 'up')\")\n",
    "        \n",
    "        # Continue collecting frames if we've started\n",
    "        elif self.first_stage_up_detected:\n",
    "            self.first_rep_buffer.append(frame.copy())\n",
    "        \n",
    "        # Save first rep when counter increases from 0 to 1\n",
    "        if self.counter > prev_counter and self.counter == 1:\n",
    "            if self.video_recorder.save_rep_video(self.first_rep_buffer, self.counter):\n",
    "                logging.info(\"Frames collected from first 'up' stage to rep completion\")\n",
    "            \n",
    "            # Reset first rep variables\n",
    "            self.first_rep_buffer = []\n",
    "            self.is_first_rep = False\n",
    "            logging.info(\"Switching to normal rep recording mode\")\n",
    "    \n",
    "    def _handle_normal_rep_recording(self, frame: np.ndarray, prev_counter: int):\n",
    "        \"\"\"Handle frame recording for normal repetitions (2+).\"\"\"\n",
    "        # Always add current frame to buffer\n",
    "        self.frame_buffer.append(frame.copy())\n",
    "        \n",
    "        # Keep buffer size manageable\n",
    "        if len(self.frame_buffer) > self.config.buffer_size:\n",
    "            self.frame_buffer.pop(0)\n",
    "        \n",
    "        # Save rep when counter increases\n",
    "        if self.counter > prev_counter:\n",
    "            self.video_recorder.save_rep_video(self.frame_buffer, self.counter)\n",
    "            self.frame_buffer = []  # Reset buffer for next rep\n",
    "    \n",
    "    def _add_angle_display(self, frame: np.ndarray, elbow_angle: float, percent: float):\n",
    "        \"\"\"Add angle and percentage display to frame.\"\"\"\n",
    "        cv2.putText(\n",
    "            frame, \n",
    "            f'Elbow: {int(elbow_angle)}', \n",
    "            (10, 100),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            self.config.font_scale, \n",
    "            (255, 255, 255), \n",
    "            self.config.font_thickness\n",
    "        )\n",
    "        cv2.putText(\n",
    "            frame, \n",
    "            f'{int(percent)}%', \n",
    "            (10, 140),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            1, \n",
    "            (0, 255, 0), \n",
    "            self.config.font_thickness\n",
    "        )\n",
    "    \n",
    "    def _add_ui_elements(self, frame: np.ndarray, label: str) -> np.ndarray:\n",
    "        \"\"\"Add UI elements to the frame.\"\"\"\n",
    "        # Buffer status\n",
    "        if self.is_first_rep:\n",
    "            if self.first_stage_up_detected:\n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    f'First Rep Buffer: {len(self.first_rep_buffer)}', \n",
    "                    (10, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.5, \n",
    "                    (0, 255, 255), \n",
    "                    2\n",
    "                )\n",
    "            else:\n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    'Waiting for first \"up\" stage...', \n",
    "                    (10, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.5, \n",
    "                    (255, 255, 0), \n",
    "                    2\n",
    "                )\n",
    "        else:\n",
    "            cv2.putText(\n",
    "                frame, \n",
    "                f'Buffer: {len(self.frame_buffer)}/{self.config.buffer_size}', \n",
    "                (10, 70),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.5, \n",
    "                (255, 255, 0), \n",
    "                2\n",
    "            )\n",
    "        \n",
    "        # Form label with first rep indicator\n",
    "        rep_status = \" (FIRST REP)\" if self.is_first_rep else \"\"\n",
    "        cv2.putText(\n",
    "            frame, \n",
    "            f'Form: {label}{rep_status}', \n",
    "            (10, 40),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            1.2, \n",
    "            (0, 255, 255), \n",
    "            2\n",
    "        )\n",
    "        \n",
    "        # Rep counter\n",
    "        cv2.rectangle(frame, (frame.shape[1] - 160, 0), (frame.shape[1], 80), (0, 0, 0), -1)\n",
    "        cv2.putText(\n",
    "            frame, \n",
    "            'REPS', \n",
    "            (frame.shape[1] - 150, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            0.8, \n",
    "            (255, 255, 255), \n",
    "            2\n",
    "        )\n",
    "        cv2.putText(\n",
    "            frame, \n",
    "            str(self.counter), \n",
    "            (frame.shape[1] - 140, 70),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            2, \n",
    "            (255, 255, 255), \n",
    "            4\n",
    "        )\n",
    "        \n",
    "        return frame\n",
    "\n",
    "\n",
    "class PushUpApp:\n",
    "    \"\"\"Main application class for push-up detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DetectionConfig = None):\n",
    "        self.config = config or DetectionConfig()\n",
    "        self.detector = PushUpDetector(self.config)\n",
    "        \n",
    "    def run(self, camera_index: int = 0):\n",
    "        \"\"\"Run the push-up detection application.\"\"\"\n",
    "        cap = cv2.VideoCapture(camera_index)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            logging.error(f\"Failed to open camera {camera_index}\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            with self.detector.mp_pose.Pose(\n",
    "                min_detection_confidence=self.config.min_detection_confidence,\n",
    "                min_tracking_confidence=self.config.min_tracking_confidence\n",
    "            ) as pose:\n",
    "                \n",
    "                while cap.isOpened():\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        logging.warning(\"Failed to read frame from camera\")\n",
    "                        break\n",
    "                    \n",
    "                    # Process frame with MediaPipe\n",
    "                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    rgb_frame.flags.writeable = False\n",
    "                    results = pose.process(rgb_frame)\n",
    "                    \n",
    "                    # Process with detector\n",
    "                    rgb_frame.flags.writeable = True\n",
    "                    bgr_frame = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)\n",
    "                    processed_frame, _ = self.detector.process_frame(bgr_frame, results)\n",
    "                    \n",
    "                    # Display frame\n",
    "                    cv2.imshow('Push-Up Detector', processed_frame)\n",
    "                    \n",
    "                    # Check for quit\n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during execution: {e}\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point.\"\"\"\n",
    "    # Create custom configuration if needed\n",
    "    config = DetectionConfig(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "        elbow_angle_up_threshold=165.0,\n",
    "        elbow_angle_down_threshold=95.0,\n",
    "        fps=20.0\n",
    "    )\n",
    "    \n",
    "    # Run the application\n",
    "    app = PushUpApp(config)\n",
    "    app.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82749ed",
   "metadata": {},
   "source": [
    "### *Downsampling to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9671e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    return angle if angle <= 180 else 360 - angle\n",
    "\n",
    "def extract_key_frames_and_downsample_to_30(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Extract frames between key points and downsample to 30 frames with guaranteed minimum elbow angle\n",
    "    \"\"\"\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    elbow_angles = []\n",
    "    frame_count = 0\n",
    "\n",
    "    print(f\"Analyzing frames for key points...\")\n",
    "    \n",
    "    # STEP 1: First pass - extract frames and calculate angles\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            lm = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Try both sides - use whichever is more visible\n",
    "            try:\n",
    "                # Try LEFT side first\n",
    "                left_shoulder = [lm[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, lm[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "                left_elbow = [lm[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, lm[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "                left_wrist = [lm[mp_pose.PoseLandmark.LEFT_WRIST.value].x, lm[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "                \n",
    "                left_visibility = (lm[mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility + \n",
    "                                 lm[mp_pose.PoseLandmark.LEFT_ELBOW.value].visibility + \n",
    "                                 lm[mp_pose.PoseLandmark.LEFT_WRIST.value].visibility) / 3\n",
    "                \n",
    "                # Try RIGHT side\n",
    "                right_shoulder = [lm[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, lm[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "                right_elbow = [lm[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, lm[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "                right_wrist = [lm[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, lm[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "                \n",
    "                right_visibility = (lm[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].visibility + \n",
    "                                  lm[mp_pose.PoseLandmark.RIGHT_ELBOW.value].visibility + \n",
    "                                  lm[mp_pose.PoseLandmark.RIGHT_WRIST.value].visibility) / 3\n",
    "                \n",
    "                # Use the side with better visibility\n",
    "                if left_visibility >= right_visibility and left_visibility > 0.5:\n",
    "                    shoulder, elbow, wrist = left_shoulder, left_elbow, left_wrist\n",
    "                elif right_visibility > 0.5:\n",
    "                    shoulder, elbow, wrist = right_shoulder, right_elbow, right_wrist\n",
    "                else:\n",
    "                    frame_count += 1\n",
    "                    continue\n",
    "                \n",
    "                angle = calculate_angle(shoulder, elbow, wrist)\n",
    "                elbow_angles.append((frame_count, angle))\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "    if len(elbow_angles) < 3:\n",
    "        print(f\"Not enough frames with pose landmarks ({len(elbow_angles)}). Skipping...\")\n",
    "        return False\n",
    "\n",
    "    # STEP 2: Identify key frame indices based on elbow angles\n",
    "    print(f\"Found {len(elbow_angles)} frames with pose data\")\n",
    "    \n",
    "    # Find lowest point (most bent elbow) - MUST be included\n",
    "    lowest_point = min(elbow_angles, key=lambda x: x[1])\n",
    "    lowest_frame_index = lowest_point[0]\n",
    "    \n",
    "    print(f\"Minimum elbow angle: {lowest_point[1]:.1f}Â° at frame {lowest_frame_index}\")\n",
    "\n",
    "    # Find highest points before and after lowest point\n",
    "    key_indices = []\n",
    "    \n",
    "    if lowest_frame_index > 0:\n",
    "        # Find frames with pose data before the lowest point\n",
    "        before_frames = [(idx, angle) for idx, angle in elbow_angles if idx < lowest_frame_index]\n",
    "        if before_frames:\n",
    "            highest_before = max(before_frames, key=lambda x: x[1])\n",
    "            start_frame = highest_before[0]\n",
    "            print(f\"Start frame: {start_frame} (angle: {highest_before[1]:.1f}Â°)\")\n",
    "        else:\n",
    "            start_frame = 0\n",
    "    else:\n",
    "        start_frame = 0\n",
    "\n",
    "    if lowest_frame_index < frame_count - 1:\n",
    "        # Find frames with pose data after the lowest point\n",
    "        after_frames = [(idx, angle) for idx, angle in elbow_angles if idx > lowest_frame_index]\n",
    "        if after_frames:\n",
    "            highest_after = max(after_frames, key=lambda x: x[1])\n",
    "            end_frame = highest_after[0]\n",
    "            print(f\"End frame: {end_frame} (angle: {highest_after[1]:.1f}Â°)\")\n",
    "        else:\n",
    "            end_frame = frame_count - 1\n",
    "    else:\n",
    "        end_frame = frame_count - 1\n",
    "\n",
    "    print(f\"Extracting frames from {start_frame} to {end_frame} (total: {end_frame - start_frame + 1} frames)\")\n",
    "    \n",
    "    # STEP 3: Extract all frames between key points\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    all_frames_in_range = []\n",
    "    frame_indices_in_range = []\n",
    "    \n",
    "    current_frame = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        if start_frame <= current_frame <= end_frame:\n",
    "            all_frames_in_range.append(frame)\n",
    "            frame_indices_in_range.append(current_frame)\n",
    "            \n",
    "        current_frame += 1\n",
    "        \n",
    "        # Stop if we've passed the end frame\n",
    "        if current_frame > end_frame:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "    if len(all_frames_in_range) == 0:\n",
    "        print(f\"No frames extracted in range. Skipping...\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Extracted {len(all_frames_in_range)} frames between key points\")\n",
    "    \n",
    "    # STEP 4: Downsample to exactly 30 frames while ensuring minimum angle frame is included\n",
    "    target_frames = 30\n",
    "    \n",
    "    if len(all_frames_in_range) <= target_frames:\n",
    "        # If we have 30 or fewer frames, use all of them\n",
    "        final_frames = all_frames_in_range\n",
    "        final_indices = frame_indices_in_range\n",
    "        print(f\"Using all {len(all_frames_in_range)} frames (â‰¤30)\")\n",
    "    else:\n",
    "        # Downsample but GUARANTEE the minimum elbow angle frame is included\n",
    "        \n",
    "        # Find the position of the minimum elbow angle frame in our extracted range\n",
    "        min_frame_position_in_range = lowest_frame_index - start_frame\n",
    "        \n",
    "        # Create evenly spaced indices\n",
    "        selected_positions = np.linspace(0, len(all_frames_in_range) - 1, target_frames, dtype=int)\n",
    "        \n",
    "        # Ensure the minimum elbow angle frame is included\n",
    "        if min_frame_position_in_range not in selected_positions:\n",
    "            # Replace the closest selected position with the minimum angle frame position\n",
    "            closest_idx = np.argmin(np.abs(selected_positions - min_frame_position_in_range))\n",
    "            selected_positions[closest_idx] = min_frame_position_in_range\n",
    "            selected_positions = np.sort(selected_positions)\n",
    "        \n",
    "        final_frames = [all_frames_in_range[i] for i in selected_positions]\n",
    "        final_indices = [frame_indices_in_range[i] for i in selected_positions]\n",
    "        \n",
    "        print(f\"â¬‡Downsampled from {len(all_frames_in_range)} to {len(final_frames)} frames\")\n",
    "        print(f\"Minimum elbow angle frame {lowest_frame_index} is guaranteed to be included\")\n",
    "    \n",
    "    # STEP 5: Save output video\n",
    "    if len(final_frames) > 0:\n",
    "        height, width, _ = final_frames[0].shape\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, 10.0, (width, height))\n",
    "        \n",
    "        for frame in final_frames:\n",
    "            out.write(frame)\n",
    "        \n",
    "        out.release()\n",
    "        print(f\"Saved {len(final_frames)} frames to: {output_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"No frames to save\")\n",
    "        return False\n",
    "\n",
    "# Process all videos in RepVids directory\n",
    "def process_repvids_to_repkeys():\n",
    "    \"\"\"\n",
    "    Process all videos in RepVids: Extract frames between key points â†’ Downsample to 30 frames â†’ Save to RepKeys\n",
    "    \"\"\"\n",
    "    # Input and output directories\n",
    "    input_dir = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"Deep Dive AI Summer 2025\", \"RepVids\")\n",
    "    output_dir = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"Deep Dive AI Summer 2025\", \"RepKeys\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Processing videos from: {input_dir}\")\n",
    "    print(f\"Saving processed videos to: {output_dir}\")\n",
    "    print(f\"Target: Extract frames between key points â†’ 30 frames max with guaranteed minimum elbow angle\\n\")\n",
    "    \n",
    "    # Get all video files\n",
    "    video_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
    "    \n",
    "    if not video_files:\n",
    "        print(\"No video files found in RepVids directory!\")\n",
    "        return 0, 0\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Process each video file\n",
    "    for i, video_file in enumerate(video_files, 1):\n",
    "        input_path = os.path.join(input_dir, video_file)\n",
    "        output_filename = f\"keyframes_30_{video_file}\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        print(f\"[{i}/{len(video_files)}] Processing: {video_file}\")\n",
    "        \n",
    "        try:\n",
    "            success = extract_key_frames_and_downsample_to_30(input_path, output_path)\n",
    "            if success:\n",
    "                successful += 1\n",
    "                print(f\"Success!\\n\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"Failed!\\n\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"Error: {e}\\n\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"PROCESSING COMPLETE!\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Return the counts so they can be accessed outside the function\n",
    "    return successful, failed\n",
    "\n",
    "# Run the processing\n",
    "if __name__ == \"__main__\":\n",
    "    successful, failed = process_repvids_to_repkeys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94246a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced first rep handling with stage-based frame collection\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Create RepVids directory in \"Deep Dive AI Summer 2025\" folder\n",
    "save_dir = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"Deep Dive AI Summer 2025\", \"RepVids\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize variables\n",
    "frame_buffer = []\n",
    "first_rep_buffer = []  # Special buffer for first rep\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "buffer_size = 90\n",
    "first_stage_up_detected = False  # Track when stage first becomes \"up\"\n",
    "is_first_rep = True  # Track if we're on the first rep\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def calc_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    return 360 - angle if angle > 180 else angle\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "counter = 0\n",
    "stage = None\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,\n",
    "                  min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Always add current frame to main buffer for non-first reps\n",
    "        if not is_first_rep:\n",
    "            frame_buffer.append(image.copy())\n",
    "\n",
    "        label = \"None\"\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            h, w, _ = image.shape\n",
    "\n",
    "            # Get right side landmarks\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                        landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "            wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                   landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "            knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                    landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "            ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "\n",
    "            # Visibility check\n",
    "            visible = all(landmarks[i].visibility > 0.7 for i in [\n",
    "                mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "                mp_pose.PoseLandmark.RIGHT_ANKLE.value\n",
    "            ])\n",
    "            \n",
    "            # Horizontal filter\n",
    "            locs = np.array([\n",
    "                shoulder[1] * h, elbow[1] * h, wrist[1] * h,\n",
    "                hip[1] * h, knee[1] * h, ankle[1] * h\n",
    "            ])\n",
    "            data_range = np.max(locs) - np.min(locs)\n",
    "            if data_range > 0.5 * h:\n",
    "                visible = False\n",
    "\n",
    "            if visible:\n",
    "                elbow_angle = calc_angle(shoulder, elbow, wrist)\n",
    "                back_angle = calc_angle(shoulder, hip, knee)\n",
    "                knee_angle = calc_angle(hip, knee, ankle)\n",
    "\n",
    "                # Pushup logic\n",
    "                percent = np.interp(elbow_angle, [90, 160], [0, 100])\n",
    "                \n",
    "                # Store previous counter and stage to detect changes\n",
    "                prev_counter = counter\n",
    "                prev_stage = stage\n",
    "\n",
    "                # Track full cycle: up -> down -> up\n",
    "                if elbow_angle > 165:\n",
    "                    if stage == \"down\":\n",
    "                        stage = \"up\"\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        stage = \"up\"\n",
    "                elif elbow_angle < 95 and stage == \"up\":\n",
    "                    stage = \"down\"\n",
    "\n",
    "                # FIRST REP SPECIAL HANDLING\n",
    "                if is_first_rep:\n",
    "                    # Start collecting frames when stage first becomes \"up\"\n",
    "                    if stage == \"up\" and not first_stage_up_detected:\n",
    "                        first_stage_up_detected = True\n",
    "                        first_rep_buffer = [image.copy()]  # Start with current frame\n",
    "                        print(\"ðŸ”´ First rep: Started collecting frames (stage = 'up')\")\n",
    "                    \n",
    "                    # Continue collecting frames if we've started\n",
    "                    elif first_stage_up_detected:\n",
    "                        first_rep_buffer.append(image.copy())\n",
    "                    \n",
    "                    # Save first rep when counter increases from 0 to 1\n",
    "                    if counter > prev_counter and counter == 1:\n",
    "                        filename = os.path.join(save_dir, f'rep_{counter:03d}.mp4')\n",
    "                        \n",
    "                        if len(first_rep_buffer) > 0:\n",
    "                            out = cv2.VideoWriter(filename, fourcc, 20.0, (image.shape[1], image.shape[0]))\n",
    "                            \n",
    "                            for f in first_rep_buffer:\n",
    "                                out.write(f)\n",
    "                            out.release()\n",
    "                            \n",
    "                            print(f\"âœ… Saved FIRST rep {counter} to: {filename} ({len(first_rep_buffer)} frames)\")\n",
    "                            print(f\"   Frames collected from first 'up' stage to rep completion\")\n",
    "                            \n",
    "                            # Reset first rep variables\n",
    "                            first_rep_buffer = []\n",
    "                            is_first_rep = False\n",
    "                            print(\"ðŸ”„ Switching to normal rep recording mode\")\n",
    "                        else:\n",
    "                            print(f\"âŒ No frames to save for first rep {counter}\")\n",
    "\n",
    "                # NORMAL REP HANDLING (for reps 2+)\n",
    "                elif counter > prev_counter:\n",
    "                    filename = os.path.join(save_dir, f'rep_{counter:03d}.mp4')\n",
    "                    \n",
    "                    if len(frame_buffer) > 0:\n",
    "                        out = cv2.VideoWriter(filename, fourcc, 20.0, (image.shape[1], image.shape[0]))\n",
    "                        \n",
    "                        for f in frame_buffer:\n",
    "                            out.write(f)\n",
    "                        out.release()\n",
    "                        \n",
    "                        print(f\"âœ… Saved rep {counter} to: {filename} ({len(frame_buffer)} frames)\")\n",
    "\n",
    "                        frame_buffer = []  # Reset buffer for next rep\n",
    "                    else:\n",
    "                        print(f\"âŒ No frames to save for rep {counter}\")\n",
    "\n",
    "                label = \"Pushup\"\n",
    "\n",
    "                # Draw angle + percent\n",
    "                cv2.putText(image, f'Elbow: {int(elbow_angle)}', (10, 100),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                cv2.putText(image, f'{int(percent)}%', (10, 140),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                label = \"None\"\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Draw buffer status with first rep indicator\n",
    "        if is_first_rep:\n",
    "            if first_stage_up_detected:\n",
    "                cv2.putText(image, f'First Rep Buffer: {len(first_rep_buffer)}', (10, 70),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(image, 'Waiting for first \"up\" stage...', (10, 70),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "        else:\n",
    "            cv2.putText(image, f'Buffer: {len(frame_buffer)}/{buffer_size}', (10, 70),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "\n",
    "        # Draw pose\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Label with first rep indicator\n",
    "        rep_status = \" (FIRST REP)\" if is_first_rep else \"\"\n",
    "        cv2.putText(image, f'Form: {label}{rep_status}', (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 2)\n",
    "\n",
    "        # Rep counter\n",
    "        cv2.rectangle(image, (image.shape[1] - 160, 0), (image.shape[1], 80), (0, 0, 0), -1)\n",
    "        cv2.putText(image, 'REPS', (image.shape[1] - 150, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        cv2.putText(image, str(counter), (image.shape[1] - 140, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 4)\n",
    "\n",
    "        cv2.imshow('Push-Up Detector', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633fd13c",
   "metadata": {},
   "source": [
    "### Live Feed Latest Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5d1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output_dir if not already defined\n",
    "output_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"Pushup Models\")\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "# Find all model files and get the most recent\n",
    "model_pattern = os.path.join(output_dir, 'pushup_classification*.h5')\n",
    "model_files = glob.glob(model_pattern)\n",
    "\n",
    "if model_files:\n",
    "    # Sort by modification time, get most recent\n",
    "    latest_model = max(model_files, key=os.path.getmtime)\n",
    "    model = load_model(latest_model)\n",
    "    model_name = os.path.basename(latest_model)\n",
    "    print(f\"Loaded most recent model: {model_name}\")\n",
    "else:\n",
    "    print(\"No existing model found. Training the initial model first with the code above.\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Version with Sliding Window:\n",
    "\n",
    "# No prerequisites required\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import deque\n",
    "import threading\n",
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "class LivePushupClassifier:\n",
    "    def __init__(self, model_path, frame_limit=30): # model was trained on 30 frames\n",
    "        self.model = load_model(model_path)\n",
    "        self.frame_buffer = deque(maxlen=frame_limit) # sliding window buffer of 30 frames\n",
    "        self.frame_limit = frame_limit\n",
    "        self.prediction_result = \"Initializing...\" # current prediction result\n",
    "        self.confidence = 0.0\n",
    "        self.is_predicting = False # thread-safe flag to prevent multiple predictions at once\n",
    "        \n",
    "    def preprocess_frame(self, frame):\n",
    "        # Converts raw camera frame to AI model input format\n",
    "        processed = cv2.resize(frame, (112, 112))\n",
    "        processed = processed / 255.0\n",
    "        return processed.astype('float32')\n",
    "    \n",
    "    def predict_async(self): # Asynchronous Prediction\n",
    "        # Run prediction in separate thread\n",
    "        if len(self.frame_buffer) == self.frame_limit and not self.is_predicting: # 30 frames taken + Prevent multiple predictions\n",
    "            self.is_predicting = True\n",
    "            \n",
    "            # Prepare 30 frame sequence\n",
    "            video_sequence = np.array(list(self.frame_buffer), dtype='float32') # preserves data order and ensures correct data type\n",
    "            video_sequence = np.expand_dims(video_sequence, axis=0)\n",
    "            # Add batch dimension for model input\n",
    "            # video_sequence shape: (1, 30, 112, 112, 3) bc TensorFlow/Keras expects 5D input for Conv3D\n",
    "\n",
    "            # Predict (in background thread)\n",
    "            prediction = self.model.predict(video_sequence, verbose=0) # Outputs probability array [[0.15, 0.85]] (Incorrect: 15%, Correct: 85%)\n",
    "            predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "            self.confidence = np.max(prediction) * 100\n",
    "            \n",
    "            # Update result\n",
    "            labels = ['Incorrect Form', 'Correct Form']\n",
    "            self.prediction_result = labels[predicted_class]\n",
    "            \n",
    "            self.is_predicting = False\n",
    "    \n",
    "    def add_frame(self, frame):\n",
    "        # Add frame to buffer and trigger prediction\n",
    "        processed_frame = self.preprocess_frame(frame)\n",
    "        self.frame_buffer.append(processed_frame) # Add processed frame to sliding window buffer\n",
    "        \n",
    "        # Start prediction in background thread when buffer is full(30 frames)\n",
    "        if len(self.frame_buffer) == self.frame_limit:\n",
    "            threading.Thread(target=self.predict_async, daemon=True).start()\n",
    "            # creates a new thread for prediction to avoid blocking the main thread\n",
    "            # target = the function to run in the new thread\n",
    "            # daemon=True allows the thread to close when the main program closes\n",
    "    \n",
    "    def get_result(self):\n",
    "        # Get current prediction result\n",
    "        return self.prediction_result, self.confidence\n",
    "\n",
    "# Usage\n",
    "def run_live_classifier():\n",
    "    # Define output_dir if not already defined\n",
    "    output_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"Pushup Models\")\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    # Find all model files and get the most recent\n",
    "    model_pattern = os.path.join(output_dir, 'pushup_classification*.h5')\n",
    "    model_files = glob.glob(model_pattern)\n",
    "\n",
    "    if model_files:\n",
    "        # Sort by modification time, get most recent\n",
    "        latest_model = max(model_files, key=os.path.getmtime)\n",
    "        model = load_model(latest_model)\n",
    "        model_name = os.path.basename(latest_model)\n",
    "        print(f\"Loaded most recent model: {model_name}\")\n",
    "\n",
    "        classifier = LivePushupClassifier(latest_model) # Initialize classifier with the loaded model\n",
    "\n",
    "    else:\n",
    "        print(\"No existing model found.\")\n",
    "        model = None\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    print(\"Live Pushup Form Checker - Press 'q' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Add frame to classifier\n",
    "        classifier.add_frame(frame) # add to sliding window buffer\n",
    "        \n",
    "        # Get prediction result\n",
    "        result, confidence = classifier.get_result() # gets latest prediction result\n",
    "        \n",
    "        # Display result\n",
    "        if \"Correct\" in result:\n",
    "            color = (0, 255, 0)  # Green\n",
    "        elif \"Incorrect\" in result:\n",
    "            color = (0, 0, 255)  # Red\n",
    "        else:\n",
    "            color = (255, 255, 0)  # Yellow\n",
    "        \n",
    "        # displays overlay text on frame\n",
    "        cv2.putText(frame, f\"{result}\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "        cv2.putText(frame, f\"Confidence: {confidence:.1f}%\", (10, 60), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        cv2.putText(frame, f\"Buffer: {len(classifier.frame_buffer)}/30\", (10, 90), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        # Creates Display frame window\n",
    "        cv2.imshow('Live Pushup Form Checker', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the live classifier\n",
    "run_live_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b16c0b",
   "metadata": {},
   "source": [
    "### all, hopfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b96788f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CNN model: pushup_classification_latest_improved.h5\n",
      "ðŸš€ Integrated Pushup Detection System Started\n",
      "ðŸ“¹ Live detection + ðŸ”„ Auto-processing + ðŸ¤– Real-time CNN classification\n",
      "Press 'q' to quit\n",
      "ðŸ”´ First rep: Started collecting frames\n",
      "âœ… Saved rep 1 to RepVids (150 frames)\n",
      "ðŸ”„ Switching to normal rep recording\n",
      "Processing rep_001.mp4...\n",
      "âœ… Saved rep 2 to RepVids (34 frames)\n",
      "Processing rep_002.mp4...\n",
      "Saved 30 frames to RepKeys\n",
      "âœ… Processed rep 2 to RepKeys\n",
      "âœ… Saved rep 3 to RepVids (43 frames)\n",
      "Processing rep_003.mp4...\n",
      "Saved 30 frames to RepKeys\n",
      "âœ… Processed rep 3 to RepKeys\n",
      "Saved 30 frames to RepKeys\n",
      "âœ… Processed rep 1 to RepKeys\n",
      "âœ… Saved rep 4 to RepVids (45 frames)\n",
      "Processing rep_004.mp4...\n",
      "Saved 30 frames to RepKeys\n",
      "âœ… Processed rep 4 to RepKeys\n",
      "âœ… Saved rep 5 to RepVids (46 frames)\n",
      "Processing rep_005.mp4...\n",
      "Saved 30 frames to RepKeys\n",
      "âœ… Processed rep 5 to RepKeys\n",
      "âœ… Saved rep 6 to RepVids (97 frames)\n",
      "Processing rep_006.mp4...\n",
      "âœ… Saved rep 7 to RepVids (68 frames)\n",
      "Processing rep_007.mp4...\n",
      "Saved 30 frames to RepKeys\n",
      "âœ… Processed rep 6 to RepKeys\n",
      "Saved 30 frames to RepKeys\n",
      "âœ… Processed rep 7 to RepKeys\n",
      "âœ… Saved rep 8 to RepVids (94 frames)\n",
      "Processing rep_008.mp4...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 frames to RepKeys\n",
      "âœ… Processed rep 8 to RepKeys\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Pushup Detection, Processing, and Classification System\n",
    "Combines live detection, keyframe extraction, and real-time CNN classification\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import glob\n",
    "from tensorflow.keras.models import load_model\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IntegratedConfig:\n",
    "    \"\"\"Configuration for the integrated system.\"\"\"\n",
    "    \n",
    "    # Detection parameters\n",
    "    min_detection_confidence: float = 0.5\n",
    "    min_tracking_confidence: float = 0.5\n",
    "    elbow_angle_up_threshold: float = 165.0\n",
    "    elbow_angle_down_threshold: float = 95.0\n",
    "    min_visibility_threshold: float = 0.7\n",
    "    horizontal_filter_ratio: float = 0.5\n",
    "    \n",
    "    # Video parameters\n",
    "    fps: float = 20.0\n",
    "    fourcc: str = 'mp4v'\n",
    "    buffer_size: int = 90\n",
    "    \n",
    "    # Directories\n",
    "    repvids_dir: str = \"RepVids\"\n",
    "    repkeys_dir: str = \"RepKeys\"\n",
    "    models_dir: str = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"Pushup Models\")\n",
    "    \n",
    "    # CNN parameters\n",
    "    target_frames: int = 30\n",
    "    frame_size: Tuple[int, int] = (112, 112)\n",
    "\n",
    "\n",
    "class AngleCalculator:\n",
    "    \"\"\"Calculate angles between body landmarks.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_angle(a: List[float], b: List[float], c: List[float]) -> float:\n",
    "        \"\"\"Calculate angle between three points.\"\"\"\n",
    "        a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "        radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "        angle = np.abs(radians * 180.0 / np.pi)\n",
    "        return 360 - angle if angle > 180 else angle\n",
    "\n",
    "\n",
    "class KeyFrameProcessor:\n",
    "    \"\"\"Process recorded pushup videos to extract key frames.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: IntegratedConfig):\n",
    "        self.config = config\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        \n",
    "    def extract_key_frames_and_downsample(self, input_path: str, output_path: str) -> bool:\n",
    "        \"\"\"Extract frames between key points and downsample to 30 frames.\"\"\"\n",
    "        pose = self.mp_pose.Pose(\n",
    "            min_detection_confidence=0.7, \n",
    "            min_tracking_confidence=0.7\n",
    "        )\n",
    "        \n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        elbow_angles = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        print(f\"Processing {os.path.basename(input_path)}...\")\n",
    "        \n",
    "        # STEP 1: Analyze all frames for elbow angles\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image_rgb)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                try:\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    \n",
    "                    # Try right side first, fallback to left\n",
    "                    shoulder = [landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                               landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "                    elbow = [landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                            landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "                    wrist = [landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                            landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "                    \n",
    "                    # Check visibility\n",
    "                    visibility = (landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].visibility +\n",
    "                                 landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].visibility +\n",
    "                                 landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].visibility) / 3\n",
    "                    \n",
    "                    if visibility > 0.5:\n",
    "                        angle = AngleCalculator.calculate_angle(shoulder, elbow, wrist)\n",
    "                        elbow_angles.append((frame_count, angle))\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            frame_count += 1\n",
    "            \n",
    "        cap.release()\n",
    "        pose.close()\n",
    "        \n",
    "        if len(elbow_angles) < 3:\n",
    "            print(f\"Insufficient pose data ({len(elbow_angles)} frames)\")\n",
    "            return False\n",
    "            \n",
    "        # STEP 2: Find key points\n",
    "        min_angle_frame = min(elbow_angles, key=lambda x: x[1])\n",
    "        min_frame_idx, min_angle = min_angle_frame\n",
    "        \n",
    "        # Find start and end frames\n",
    "        before_frames = [(idx, angle) for idx, angle in elbow_angles if idx < min_frame_idx]\n",
    "        after_frames = [(idx, angle) for idx, angle in elbow_angles if idx > min_frame_idx]\n",
    "        \n",
    "        start_frame = max(before_frames, key=lambda x: x[1])[0] if before_frames else 0\n",
    "        end_frame = max(after_frames, key=lambda x: x[1])[0] if after_frames else frame_count - 1\n",
    "        \n",
    "        # STEP 3: Extract and downsample frames\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        extracted_frames = []\n",
    "        current_frame = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            if start_frame <= current_frame <= end_frame:\n",
    "                extracted_frames.append(frame)\n",
    "                \n",
    "            current_frame += 1\n",
    "            if current_frame > end_frame:\n",
    "                break\n",
    "                \n",
    "        cap.release()\n",
    "        \n",
    "        if not extracted_frames:\n",
    "            return False\n",
    "            \n",
    "        # Downsample to target frames (30)\n",
    "        if len(extracted_frames) <= self.config.target_frames:\n",
    "            final_frames = extracted_frames\n",
    "        else:\n",
    "            # Ensure minimum angle frame is included\n",
    "            min_frame_pos = min_frame_idx - start_frame\n",
    "            indices = np.linspace(0, len(extracted_frames) - 1, self.config.target_frames, dtype=int)\n",
    "            \n",
    "            if min_frame_pos not in indices:\n",
    "                closest_idx = np.argmin(np.abs(indices - min_frame_pos))\n",
    "                indices[closest_idx] = min_frame_pos\n",
    "                indices = np.sort(indices)\n",
    "                \n",
    "            final_frames = [extracted_frames[i] for i in indices]\n",
    "            \n",
    "        # STEP 4: Save output\n",
    "        if final_frames:\n",
    "            height, width = final_frames[0].shape[:2]\n",
    "            fourcc = cv2.VideoWriter_fourcc(*self.config.fourcc)\n",
    "            out = cv2.VideoWriter(output_path, fourcc, 10.0, (width, height))\n",
    "            \n",
    "            for frame in final_frames:\n",
    "                out.write(frame)\n",
    "            out.release()\n",
    "            \n",
    "            print(f\"Saved {len(final_frames)} frames to RepKeys\")\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "\n",
    "class CNNClassifier:\n",
    "    \"\"\"Real-time CNN classification with threading.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: IntegratedConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.frame_buffer = deque(maxlen=config.target_frames)\n",
    "        self.prediction_result = \"Initializing...\"\n",
    "        self.confidence = 0.0\n",
    "        self.is_predicting = False\n",
    "        self.load_latest_model()\n",
    "        \n",
    "    def load_latest_model(self):\n",
    "        \"\"\"Load the most recent CNN model.\"\"\"\n",
    "        os.makedirs(self.config.models_dir, exist_ok=True)\n",
    "        model_pattern = os.path.join(self.config.models_dir, 'pushup_classification*.h5')\n",
    "        model_files = glob.glob(model_pattern)\n",
    "        \n",
    "        if model_files:\n",
    "            latest_model = max(model_files, key=os.path.getmtime)\n",
    "            self.model = load_model(latest_model)\n",
    "            model_name = os.path.basename(latest_model)\n",
    "            print(f\"Loaded CNN model: {model_name}\")\n",
    "        else:\n",
    "            print(\"No CNN model found - classification disabled\")\n",
    "            \n",
    "    def preprocess_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess frame for CNN input.\"\"\"\n",
    "        processed = cv2.resize(frame, self.config.frame_size)\n",
    "        processed = processed / 255.0\n",
    "        return processed.astype('float32')\n",
    "        \n",
    "    def predict_async(self):\n",
    "        \"\"\"Run prediction in background thread.\"\"\"\n",
    "        if (len(self.frame_buffer) == self.config.target_frames and \n",
    "            not self.is_predicting and self.model is not None):\n",
    "            \n",
    "            self.is_predicting = True\n",
    "            \n",
    "            try:\n",
    "                # Prepare sequence for CNN\n",
    "                video_sequence = np.array(list(self.frame_buffer), dtype='float32')\n",
    "                video_sequence = np.expand_dims(video_sequence, axis=0)\n",
    "                \n",
    "                # Predict\n",
    "                prediction = self.model.predict(video_sequence, verbose=0)\n",
    "                predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "                self.confidence = np.max(prediction) * 100\n",
    "                \n",
    "                # Update result\n",
    "                labels = ['Incorrect Form', 'Correct Form']\n",
    "                self.prediction_result = labels[predicted_class]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {e}\")\n",
    "                self.prediction_result = \"Error\"\n",
    "                self.confidence = 0.0\n",
    "                \n",
    "            finally:\n",
    "                self.is_predicting = False\n",
    "                \n",
    "    def add_frame(self, frame: np.ndarray):\n",
    "        \"\"\"Add frame to buffer and trigger prediction.\"\"\"\n",
    "        processed_frame = self.preprocess_frame(frame)\n",
    "        self.frame_buffer.append(processed_frame)\n",
    "        \n",
    "        if len(self.frame_buffer) == self.config.target_frames:\n",
    "            threading.Thread(target=self.predict_async, daemon=True).start()\n",
    "            \n",
    "    def get_result(self) -> Tuple[str, float]:\n",
    "        \"\"\"Get current prediction result.\"\"\"\n",
    "        return self.prediction_result, self.confidence\n",
    "\n",
    "\n",
    "class IntegratedPushupDetector:\n",
    "    \"\"\"Main integrated pushup detection system.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: IntegratedConfig = None):\n",
    "        self.config = config or IntegratedConfig()\n",
    "        \n",
    "        # Setup directories\n",
    "        self.repvids_path = Path.home() / \"Desktop\" / \"Deep Dive AI Summer 2025\" / self.config.repvids_dir\n",
    "        self.repkeys_path = Path.home() / \"Desktop\" / \"Deep Dive AI Summer 2025\" / self.config.repkeys_dir\n",
    "        self.repvids_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.repkeys_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.keyframe_processor = KeyFrameProcessor(self.config)\n",
    "        self.cnn_classifier = CNNClassifier(self.config)\n",
    "        \n",
    "        # MediaPipe setup\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        \n",
    "        # Detection state\n",
    "        self.counter = 0\n",
    "        self.stage = None\n",
    "        self.frame_buffer = []\n",
    "        self.first_rep_buffer = []\n",
    "        self.first_stage_up_detected = False\n",
    "        self.is_first_rep = True\n",
    "        \n",
    "        # Video recording\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*self.config.fourcc)\n",
    "        \n",
    "    def extract_landmarks(self, results) -> Optional[Dict[str, List[float]]]:\n",
    "        \"\"\"Extract pose landmarks.\"\"\"\n",
    "        if not results.pose_landmarks:\n",
    "            return None\n",
    "            \n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        return {\n",
    "            'shoulder': [landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                        landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "            'elbow': [landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                     landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].y],\n",
    "            'wrist': [landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                     landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].y],\n",
    "            'hip': [landmarks[self.mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                   landmarks[self.mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "            'knee': [landmarks[self.mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                    landmarks[self.mp_pose.PoseLandmark.RIGHT_KNEE.value].y],\n",
    "            'ankle': [landmarks[self.mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                     landmarks[self.mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "        }\n",
    "        \n",
    "    def is_pose_valid(self, results, landmark_dict: Dict, image_height: int) -> bool:\n",
    "        \"\"\"Check if pose is valid for detection.\"\"\"\n",
    "        if not results.pose_landmarks:\n",
    "            return False\n",
    "            \n",
    "        # Visibility check\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        required_landmarks = [\n",
    "            self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            self.mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            self.mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "            self.mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "            self.mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            self.mp_pose.PoseLandmark.RIGHT_ANKLE.value\n",
    "        ]\n",
    "        \n",
    "        if not all(landmarks[i].visibility > self.config.min_visibility_threshold \n",
    "                  for i in required_landmarks):\n",
    "            return False\n",
    "            \n",
    "        # Horizontal filter\n",
    "        y_coords = [\n",
    "            landmark_dict['shoulder'][1] * image_height,\n",
    "            landmark_dict['elbow'][1] * image_height,\n",
    "            landmark_dict['wrist'][1] * image_height,\n",
    "            landmark_dict['hip'][1] * image_height,\n",
    "            landmark_dict['knee'][1] * image_height,\n",
    "            landmark_dict['ankle'][1] * image_height\n",
    "        ]\n",
    "        \n",
    "        data_range = max(y_coords) - min(y_coords)\n",
    "        return data_range <= self.config.horizontal_filter_ratio * image_height\n",
    "        \n",
    "    def process_rep_completion(self, frame: np.ndarray, rep_number: int, frames: List[np.ndarray]):\n",
    "        \"\"\"Process completed repetition.\"\"\"\n",
    "        if not frames:\n",
    "            print(f\"No frames to save for rep {rep_number}\")\n",
    "            return\n",
    "            \n",
    "        # Save to RepVids\n",
    "        repvids_filename = self.repvids_path / f'rep_{rep_number:03d}.mp4'\n",
    "        height, width = frame.shape[:2]\n",
    "        out = cv2.VideoWriter(str(repvids_filename), self.fourcc, self.config.fps, (width, height))\n",
    "        \n",
    "        for f in frames:\n",
    "            out.write(f)\n",
    "        out.release()\n",
    "        \n",
    "        print(f\"âœ… Saved rep {rep_number} to RepVids ({len(frames)} frames)\")\n",
    "        \n",
    "        # Process to RepKeys in background thread\n",
    "        def process_to_repkeys():\n",
    "            repkeys_filename = self.repkeys_path / f'keyframes_30_rep_{rep_number:03d}.mp4'\n",
    "            success = self.keyframe_processor.extract_key_frames_and_downsample(\n",
    "                str(repvids_filename), str(repkeys_filename)\n",
    "            )\n",
    "            if success:\n",
    "                print(f\"âœ… Processed rep {rep_number} to RepKeys\")\n",
    "            else:\n",
    "                print(f\"âŒ Failed to process rep {rep_number} to RepKeys\")\n",
    "                \n",
    "        threading.Thread(target=process_to_repkeys, daemon=True).start()\n",
    "        \n",
    "    def run(self, camera_index: int = 0):\n",
    "        \"\"\"Run the integrated detection system.\"\"\"\n",
    "        cap = cv2.VideoCapture(camera_index)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open camera {camera_index}\")\n",
    "            return\n",
    "            \n",
    "        print(\"ðŸš€ Integrated Pushup Detection System Started\")\n",
    "        print(\"ðŸ“¹ Live detection + ðŸ”„ Auto-processing + ðŸ¤– Real-time CNN classification\")\n",
    "        print(\"Press 'q' to quit\")\n",
    "        \n",
    "        try:\n",
    "            with self.mp_pose.Pose(\n",
    "                min_detection_confidence=self.config.min_detection_confidence,\n",
    "                min_tracking_confidence=self.config.min_tracking_confidence\n",
    "            ) as pose:\n",
    "                \n",
    "                while cap.isOpened():\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                        \n",
    "                    # Process with MediaPipe\n",
    "                    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    image.flags.writeable = False\n",
    "                    results = pose.process(image)\n",
    "                    \n",
    "                    image.flags.writeable = True\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Add frame to CNN classifier for real-time feedback\n",
    "                    self.cnn_classifier.add_frame(image)\n",
    "                    \n",
    "                    # Handle frame buffering for rep recording\n",
    "                    if not self.is_first_rep:\n",
    "                        self.frame_buffer.append(image.copy())\n",
    "                        \n",
    "                    # Extract landmarks and process pushup detection\n",
    "                    landmark_dict = self.extract_landmarks(results)\n",
    "                    label = \"None\"\n",
    "                    \n",
    "                    if landmark_dict and self.is_pose_valid(results, landmark_dict, image.shape[0]):\n",
    "                        elbow_angle = AngleCalculator.calculate_angle(\n",
    "                            landmark_dict['shoulder'], \n",
    "                            landmark_dict['elbow'], \n",
    "                            landmark_dict['wrist']\n",
    "                        )\n",
    "                        \n",
    "                        # Update detection state\n",
    "                        prev_counter = self.counter\n",
    "                        \n",
    "                        if elbow_angle > self.config.elbow_angle_up_threshold:\n",
    "                            if self.stage == \"down\":\n",
    "                                self.stage = \"up\"\n",
    "                                self.counter += 1\n",
    "                            else:\n",
    "                                self.stage = \"up\"\n",
    "                        elif elbow_angle < self.config.elbow_angle_down_threshold and self.stage == \"up\":\n",
    "                            self.stage = \"down\"\n",
    "                            \n",
    "                        # Handle rep recording\n",
    "                        if self.is_first_rep:\n",
    "                            # First rep handling\n",
    "                            if self.stage == \"up\" and not self.first_stage_up_detected:\n",
    "                                self.first_stage_up_detected = True\n",
    "                                self.first_rep_buffer = [image.copy()]\n",
    "                                print(\"ðŸ”´ First rep: Started collecting frames\")\n",
    "                            elif self.first_stage_up_detected:\n",
    "                                self.first_rep_buffer.append(image.copy())\n",
    "                                \n",
    "                            if self.counter > prev_counter and self.counter == 1:\n",
    "                                self.process_rep_completion(image, self.counter, self.first_rep_buffer)\n",
    "                                self.first_rep_buffer = []\n",
    "                                self.is_first_rep = False\n",
    "                                print(\"ðŸ”„ Switching to normal rep recording\")\n",
    "                                \n",
    "                        elif self.counter > prev_counter:\n",
    "                            # Normal rep handling\n",
    "                            self.process_rep_completion(image, self.counter, self.frame_buffer)\n",
    "                            self.frame_buffer = []\n",
    "                            \n",
    "                        label = \"Pushup\"\n",
    "                        \n",
    "                        # Display angle info\n",
    "                        percent = np.interp(elbow_angle, [90, 160], [0, 100])\n",
    "                        cv2.putText(image, f'Elbow: {int(elbow_angle)}Â°', (10, 100),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                        cv2.putText(image, f'{int(percent)}%', (10, 140),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                                   \n",
    "                    # Get CNN classification result\n",
    "                    cnn_result, cnn_confidence = self.cnn_classifier.get_result()\n",
    "                    \n",
    "                    # Display CNN feedback\n",
    "                    if \"Correct\" in cnn_result:\n",
    "                        cnn_color = (0, 255, 0)  # Green\n",
    "                    elif \"Incorrect\" in cnn_result:\n",
    "                        cnn_color = (0, 0, 255)  # Red\n",
    "                    else:\n",
    "                        cnn_color = (255, 255, 0)  # Yellow\n",
    "                        \n",
    "                    cv2.putText(image, f\"Form: {cnn_result}\", (10, 170),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, cnn_color, 2)\n",
    "                    cv2.putText(image, f\"Confidence: {cnn_confidence:.1f}%\", (10, 200),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, cnn_color, 2)\n",
    "                               \n",
    "                    # Buffer status\n",
    "                    if self.is_first_rep:\n",
    "                        if self.first_stage_up_detected:\n",
    "                            cv2.putText(image, f'First Rep Buffer: {len(self.first_rep_buffer)}', \n",
    "                                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "                        else:\n",
    "                            cv2.putText(image, 'Waiting for first \"up\" stage...', \n",
    "                                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "                    else:\n",
    "                        cv2.putText(image, f'Buffer: {len(self.frame_buffer)}/{self.config.buffer_size}', \n",
    "                                   (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "                                   \n",
    "                    # Draw pose landmarks\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        image, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=4),\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "                    )\n",
    "                    \n",
    "                    # Status and rep counter\n",
    "                    rep_status = \" (FIRST REP)\" if self.is_first_rep else \"\"\n",
    "                    cv2.putText(image, f'Detection: {label}{rep_status}', (10, 40),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "                               \n",
    "                    # Rep counter\n",
    "                    cv2.rectangle(image, (image.shape[1] - 160, 0), (image.shape[1], 80), (0, 0, 0), -1)\n",
    "                    cv2.putText(image, 'REPS', (image.shape[1] - 150, 30),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "                    cv2.putText(image, str(self.counter), (image.shape[1] - 140, 70),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 4)\n",
    "                               \n",
    "                    # CNN buffer status\n",
    "                    cv2.putText(image, f'CNN Buffer: {len(self.cnn_classifier.frame_buffer)}/30', \n",
    "                               (10, 230), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "                               \n",
    "                    cv2.imshow('Integrated Pushup Detection System', image)\n",
    "                    \n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during execution: {e}\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the integrated pushup detection system.\"\"\"\n",
    "    config = IntegratedConfig(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "        elbow_angle_up_threshold=165.0,\n",
    "        elbow_angle_down_threshold=95.0,\n",
    "        target_frames=30\n",
    "    )\n",
    "    \n",
    "    detector = IntegratedPushupDetector(config)\n",
    "    detector.run()\n",
    "\n",
    "\n",
    "# Run the integrated system\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8bc0b",
   "metadata": {},
   "source": [
    "# dfbfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c997a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CNN model: pushup_classification_latest_improved.h5\n",
      "ðŸš€ Integrated Pushup Detection System Started\n",
      "ðŸ“¹ Live detection + ðŸ”„ Auto-processing + ðŸ¤– Real-time CNN classification\n",
      "ðŸ” Automatic bilateral landmark detection (left/right)\n",
      "Press 'q' to quit\n",
      "ðŸ”´ First rep: Started collecting frames\n",
      "âœ… Saved rep 1 to RepVids (129 frames)\n",
      "ðŸ”„ Switching to normal rep recording\n",
      "Processing rep_001.mp4...\n",
      "âœ… Saved rep 2 to RepVids (35 frames)\n",
      "Processing rep_002.mp4...\n",
      "Saved 21 frames to RepKeys\n",
      "âœ… Processed rep 2 to RepKeys\n",
      "âœ… Saved rep 3 to RepVids (39 frames)\n",
      "Processing rep_003.mp4...\n",
      "Saved 30 frames to RepKeys\n",
      "âœ… Processed rep 1 to RepKeys\n",
      "âœ… Saved rep 4 to RepVids (37 frames)\n",
      "Processing rep_004.mp4...\n",
      "Saved 30 frames to RepKeys\n",
      "âœ… Processed rep 3 to RepKeys\n",
      "Saved 30 frames to RepKeys\n",
      "âœ… Processed rep 4 to RepKeys\n",
      "âœ… Saved rep 5 to RepVids (42 frames)\n",
      "Processing rep_005.mp4...\n",
      "Saved 20 frames to RepKeys\n",
      "âœ… Processed rep 5 to RepKeys\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Pushup Detection, Processing, and Classification System\n",
    "Combines live detection, keyframe extraction, and real-time CNN classification\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import glob\n",
    "from tensorflow.keras.models import load_model\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IntegratedConfig:\n",
    "    \"\"\"Configuration for the integrated system.\"\"\"\n",
    "    \n",
    "    # Detection parameters\n",
    "    min_detection_confidence: float = 0.5\n",
    "    min_tracking_confidence: float = 0.5\n",
    "    elbow_angle_up_threshold: float = 165.0\n",
    "    elbow_angle_down_threshold: float = 95.0\n",
    "    min_visibility_threshold: float = 0.7\n",
    "    horizontal_filter_ratio: float = 0.5\n",
    "    \n",
    "    # Video parameters\n",
    "    fps: float = 20.0\n",
    "    fourcc: str = 'mp4v'\n",
    "    buffer_size: int = 90\n",
    "    \n",
    "    # Directories\n",
    "    repvids_dir: str = \"RepVids\"\n",
    "    repkeys_dir: str = \"RepKeys\"\n",
    "    models_dir: str = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"Pushup Models\")\n",
    "    \n",
    "    # CNN parameters\n",
    "    target_frames: int = 30\n",
    "    frame_size: Tuple[int, int] = (112, 112)\n",
    "\n",
    "\n",
    "class AngleCalculator:\n",
    "    \"\"\"Calculate angles between body landmarks.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_angle(a: List[float], b: List[float], c: List[float]) -> float:\n",
    "        \"\"\"Calculate angle between three points.\"\"\"\n",
    "        a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "        radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "        angle = np.abs(radians * 180.0 / np.pi)\n",
    "        return 360 - angle if angle > 180 else angle\n",
    "\n",
    "\n",
    "class KeyFrameProcessor:\n",
    "    \"\"\"Process recorded pushup videos to extract key frames.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: IntegratedConfig):\n",
    "        self.config = config\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        \n",
    "    def extract_key_frames_and_downsample(self, input_path: str, output_path: str) -> bool:\n",
    "        \"\"\"Extract frames between key points and downsample to 30 frames.\"\"\"\n",
    "        pose = self.mp_pose.Pose(\n",
    "            min_detection_confidence=0.7, \n",
    "            min_tracking_confidence=0.7\n",
    "        )\n",
    "        \n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        elbow_angles = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        print(f\"Processing {os.path.basename(input_path)}...\")\n",
    "        \n",
    "        # STEP 1: Analyze all frames for elbow angles\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image_rgb)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                try:\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    \n",
    "                    # Try both sides - use whichever is more visible\n",
    "                    # LEFT side\n",
    "                    left_shoulder = [landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                                   landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "                    left_elbow = [landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                                landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "                    left_wrist = [landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                                landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "                    \n",
    "                    left_visibility = (landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility +\n",
    "                                     landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW.value].visibility +\n",
    "                                     landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value].visibility) / 3\n",
    "                    \n",
    "                    # RIGHT side\n",
    "                    right_shoulder = [landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                                    landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "                    right_elbow = [landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                                 landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "                    right_wrist = [landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                                 landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "                    \n",
    "                    right_visibility = (landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].visibility +\n",
    "                                      landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].visibility +\n",
    "                                      landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].visibility) / 3\n",
    "                    \n",
    "                    # Use the side with better visibility\n",
    "                    if left_visibility >= right_visibility and left_visibility > 0.5:\n",
    "                        shoulder, elbow, wrist = left_shoulder, left_elbow, left_wrist\n",
    "                    elif right_visibility > 0.5:\n",
    "                        shoulder, elbow, wrist = right_shoulder, right_elbow, right_wrist\n",
    "                    else:\n",
    "                        frame_count += 1\n",
    "                        continue\n",
    "                    \n",
    "                    angle = AngleCalculator.calculate_angle(shoulder, elbow, wrist)\n",
    "                    elbow_angles.append((frame_count, angle))\n",
    "                    \n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            frame_count += 1\n",
    "            \n",
    "        cap.release()\n",
    "        pose.close()\n",
    "        \n",
    "        if len(elbow_angles) < 3:\n",
    "            print(f\"Insufficient pose data ({len(elbow_angles)} frames)\")\n",
    "            return False\n",
    "            \n",
    "        # STEP 2: Find key points\n",
    "        min_angle_frame = min(elbow_angles, key=lambda x: x[1])\n",
    "        min_frame_idx, min_angle = min_angle_frame\n",
    "        \n",
    "        # Find start and end frames\n",
    "        before_frames = [(idx, angle) for idx, angle in elbow_angles if idx < min_frame_idx]\n",
    "        after_frames = [(idx, angle) for idx, angle in elbow_angles if idx > min_frame_idx]\n",
    "        \n",
    "        start_frame = max(before_frames, key=lambda x: x[1])[0] if before_frames else 0\n",
    "        end_frame = max(after_frames, key=lambda x: x[1])[0] if after_frames else frame_count - 1\n",
    "        \n",
    "        # STEP 3: Extract and downsample frames\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        extracted_frames = []\n",
    "        current_frame = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            if start_frame <= current_frame <= end_frame:\n",
    "                extracted_frames.append(frame)\n",
    "                \n",
    "            current_frame += 1\n",
    "            if current_frame > end_frame:\n",
    "                break\n",
    "                \n",
    "        cap.release()\n",
    "        \n",
    "        if not extracted_frames:\n",
    "            return False\n",
    "            \n",
    "        # Downsample to target frames (30)\n",
    "        if len(extracted_frames) <= self.config.target_frames:\n",
    "            final_frames = extracted_frames\n",
    "        else:\n",
    "            # Ensure minimum angle frame is included\n",
    "            min_frame_pos = min_frame_idx - start_frame\n",
    "            indices = np.linspace(0, len(extracted_frames) - 1, self.config.target_frames, dtype=int)\n",
    "            \n",
    "            if min_frame_pos not in indices:\n",
    "                closest_idx = np.argmin(np.abs(indices - min_frame_pos))\n",
    "                indices[closest_idx] = min_frame_pos\n",
    "                indices = np.sort(indices)\n",
    "                \n",
    "            final_frames = [extracted_frames[i] for i in indices]\n",
    "            \n",
    "        # STEP 4: Save output\n",
    "        if final_frames:\n",
    "            height, width = final_frames[0].shape[:2]\n",
    "            fourcc = cv2.VideoWriter_fourcc(*self.config.fourcc)\n",
    "            out = cv2.VideoWriter(output_path, fourcc, 10.0, (width, height))\n",
    "            \n",
    "            for frame in final_frames:\n",
    "                out.write(frame)\n",
    "            out.release()\n",
    "            \n",
    "            print(f\"Saved {len(final_frames)} frames to RepKeys\")\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "\n",
    "class CNNClassifier:\n",
    "    \"\"\"Real-time CNN classification with threading.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: IntegratedConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.frame_buffer = deque(maxlen=config.target_frames)\n",
    "        self.prediction_result = \"Initializing...\"\n",
    "        self.confidence = 0.0\n",
    "        self.is_predicting = False\n",
    "        self.load_latest_model()\n",
    "        \n",
    "    def load_latest_model(self):\n",
    "        \"\"\"Load the most recent CNN model.\"\"\"\n",
    "        os.makedirs(self.config.models_dir, exist_ok=True)\n",
    "        model_pattern = os.path.join(self.config.models_dir, 'pushup_classification*.h5')\n",
    "        model_files = glob.glob(model_pattern)\n",
    "        \n",
    "        if model_files:\n",
    "            latest_model = max(model_files, key=os.path.getmtime)\n",
    "            self.model = load_model(latest_model)\n",
    "            model_name = os.path.basename(latest_model)\n",
    "            print(f\"Loaded CNN model: {model_name}\")\n",
    "        else:\n",
    "            print(\"No CNN model found - classification disabled\")\n",
    "            \n",
    "    def preprocess_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess frame for CNN input.\"\"\"\n",
    "        processed = cv2.resize(frame, self.config.frame_size)\n",
    "        processed = processed / 255.0\n",
    "        return processed.astype('float32')\n",
    "        \n",
    "    def predict_async(self):\n",
    "        \"\"\"Run prediction in background thread.\"\"\"\n",
    "        if (len(self.frame_buffer) == self.config.target_frames and \n",
    "            not self.is_predicting and self.model is not None):\n",
    "            \n",
    "            self.is_predicting = True\n",
    "            \n",
    "            try:\n",
    "                # Prepare sequence for CNN\n",
    "                video_sequence = np.array(list(self.frame_buffer), dtype='float32')\n",
    "                video_sequence = np.expand_dims(video_sequence, axis=0)\n",
    "                \n",
    "                # Predict\n",
    "                prediction = self.model.predict(video_sequence, verbose=0)\n",
    "                predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "                self.confidence = np.max(prediction) * 100\n",
    "                \n",
    "                # Update result\n",
    "                labels = ['Incorrect Form', 'Correct Form']\n",
    "                self.prediction_result = labels[predicted_class]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {e}\")\n",
    "                self.prediction_result = \"Error\"\n",
    "                self.confidence = 0.0\n",
    "                \n",
    "            finally:\n",
    "                self.is_predicting = False\n",
    "                \n",
    "    def add_frame(self, frame: np.ndarray):\n",
    "        \"\"\"Add frame to buffer and trigger prediction.\"\"\"\n",
    "        processed_frame = self.preprocess_frame(frame)\n",
    "        self.frame_buffer.append(processed_frame)\n",
    "        \n",
    "        if len(self.frame_buffer) == self.config.target_frames:\n",
    "            threading.Thread(target=self.predict_async, daemon=True).start()\n",
    "            \n",
    "    def get_result(self) -> Tuple[str, float]:\n",
    "        \"\"\"Get current prediction result.\"\"\"\n",
    "        return self.prediction_result, self.confidence\n",
    "\n",
    "\n",
    "class IntegratedPushupDetector:\n",
    "    \"\"\"Main integrated pushup detection system.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: IntegratedConfig = None):\n",
    "        self.config = config or IntegratedConfig()\n",
    "        \n",
    "        # Setup directories\n",
    "        self.repvids_path = Path.home() / \"Desktop\" / \"Deep Dive AI Summer 2025\" / self.config.repvids_dir\n",
    "        self.repkeys_path = Path.home() / \"Desktop\" / \"Deep Dive AI Summer 2025\" / self.config.repkeys_dir\n",
    "        self.repvids_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.repkeys_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.keyframe_processor = KeyFrameProcessor(self.config)\n",
    "        self.cnn_classifier = CNNClassifier(self.config)\n",
    "        \n",
    "        # MediaPipe setup\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        \n",
    "        # Detection state\n",
    "        self.counter = 0\n",
    "        self.stage = None\n",
    "        self.frame_buffer = []\n",
    "        self.first_rep_buffer = []\n",
    "        self.first_stage_up_detected = False\n",
    "        self.is_first_rep = True\n",
    "        \n",
    "        # Video recording\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*self.config.fourcc)\n",
    "        \n",
    "    def extract_landmarks(self, results) -> Optional[Dict[str, List[float]]]:\n",
    "        \"\"\"Extract pose landmarks - uses better visible side.\"\"\"\n",
    "        if not results.pose_landmarks:\n",
    "            return None\n",
    "            \n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        try:\n",
    "            # Get LEFT side landmarks\n",
    "            left_landmarks = {\n",
    "                'shoulder': [landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                           landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                'elbow': [landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                         landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW.value].y],\n",
    "                'wrist': [landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                         landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value].y],\n",
    "                'hip': [landmarks[self.mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                       landmarks[self.mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                'knee': [landmarks[self.mp_pose.PoseLandmark.LEFT_KNEE.value].x,\n",
    "                        landmarks[self.mp_pose.PoseLandmark.LEFT_KNEE.value].y],\n",
    "                'ankle': [landmarks[self.mp_pose.PoseLandmark.LEFT_ANKLE.value].x,\n",
    "                         landmarks[self.mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "            }\n",
    "            \n",
    "            # Calculate LEFT side visibility\n",
    "            left_visibility = (\n",
    "                landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility +\n",
    "                landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW.value].visibility +\n",
    "                landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value].visibility +\n",
    "                landmarks[self.mp_pose.PoseLandmark.LEFT_HIP.value].visibility +\n",
    "                landmarks[self.mp_pose.PoseLandmark.LEFT_KNEE.value].visibility +\n",
    "                landmarks[self.mp_pose.PoseLandmark.LEFT_ANKLE.value].visibility\n",
    "            ) / 6\n",
    "            \n",
    "            # Get RIGHT side landmarks\n",
    "            right_landmarks = {\n",
    "                'shoulder': [landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                           landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                'elbow': [landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                         landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].y],\n",
    "                'wrist': [landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                         landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].y],\n",
    "                'hip': [landmarks[self.mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                       landmarks[self.mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                'knee': [landmarks[self.mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                        landmarks[self.mp_pose.PoseLandmark.RIGHT_KNEE.value].y],\n",
    "                'ankle': [landmarks[self.mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                         landmarks[self.mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "            }\n",
    "            \n",
    "            # Calculate RIGHT side visibility\n",
    "            right_visibility = (\n",
    "                landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].visibility +\n",
    "                landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].visibility +\n",
    "                landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].visibility +\n",
    "                landmarks[self.mp_pose.PoseLandmark.RIGHT_HIP.value].visibility +\n",
    "                landmarks[self.mp_pose.PoseLandmark.RIGHT_KNEE.value].visibility +\n",
    "                landmarks[self.mp_pose.PoseLandmark.RIGHT_ANKLE.value].visibility\n",
    "            ) / 6\n",
    "            \n",
    "            # Return the side with better visibility\n",
    "            if left_visibility >= right_visibility:\n",
    "                return left_landmarks\n",
    "            else:\n",
    "                return right_landmarks\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting landmarks: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def is_pose_valid(self, results, landmark_dict: Dict, image_height: int) -> bool:\n",
    "        \"\"\"Check if pose is valid for detection - works with both sides.\"\"\"\n",
    "        if not results.pose_landmarks or not landmark_dict:\n",
    "            return False\n",
    "            \n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        # Determine which side we're using based on the landmark_dict\n",
    "        # Check if we have left or right side landmarks\n",
    "        shoulder_x = landmark_dict['shoulder'][0]\n",
    "        \n",
    "        # Find which side has better visibility by checking both sides\n",
    "        left_landmarks = [\n",
    "            self.mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "            self.mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            self.mp_pose.PoseLandmark.LEFT_WRIST.value,\n",
    "            self.mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "            self.mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "            self.mp_pose.PoseLandmark.LEFT_ANKLE.value\n",
    "        ]\n",
    "        \n",
    "        right_landmarks = [\n",
    "            self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            self.mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            self.mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "            self.mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "            self.mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            self.mp_pose.PoseLandmark.RIGHT_ANKLE.value\n",
    "        ]\n",
    "        \n",
    "        # Calculate visibility for both sides\n",
    "        left_visibility = sum(landmarks[i].visibility for i in left_landmarks) / len(left_landmarks)\n",
    "        right_visibility = sum(landmarks[i].visibility for i in right_landmarks) / len(right_landmarks)\n",
    "        \n",
    "        # Use the side with better visibility for validation\n",
    "        if left_visibility >= right_visibility:\n",
    "            required_landmarks = left_landmarks\n",
    "            current_visibility = left_visibility\n",
    "        else:\n",
    "            required_landmarks = right_landmarks\n",
    "            current_visibility = right_visibility\n",
    "        \n",
    "        # Visibility check\n",
    "        if current_visibility <= self.config.min_visibility_threshold:\n",
    "            return False\n",
    "            \n",
    "        # Horizontal filter\n",
    "        y_coords = [\n",
    "            landmark_dict['shoulder'][1] * image_height,\n",
    "            landmark_dict['elbow'][1] * image_height,\n",
    "            landmark_dict['wrist'][1] * image_height,\n",
    "            landmark_dict['hip'][1] * image_height,\n",
    "            landmark_dict['knee'][1] * image_height,\n",
    "            landmark_dict['ankle'][1] * image_height\n",
    "        ]\n",
    "        \n",
    "        data_range = max(y_coords) - min(y_coords)\n",
    "        return data_range <= self.config.horizontal_filter_ratio * image_height\n",
    "        \n",
    "    def process_rep_completion(self, frame: np.ndarray, rep_number: int, frames: List[np.ndarray]):\n",
    "        \"\"\"Process completed repetition.\"\"\"\n",
    "        if not frames:\n",
    "            print(f\"No frames to save for rep {rep_number}\")\n",
    "            return\n",
    "            \n",
    "        # Save to RepVids\n",
    "        repvids_filename = self.repvids_path / f'rep_{rep_number:03d}.mp4'\n",
    "        height, width = frame.shape[:2]\n",
    "        out = cv2.VideoWriter(str(repvids_filename), self.fourcc, self.config.fps, (width, height))\n",
    "        \n",
    "        for f in frames:\n",
    "            out.write(f)\n",
    "        out.release()\n",
    "        \n",
    "        print(f\"âœ… Saved rep {rep_number} to RepVids ({len(frames)} frames)\")\n",
    "        \n",
    "        # Process to RepKeys in background thread\n",
    "        def process_to_repkeys():\n",
    "            repkeys_filename = self.repkeys_path / f'keyframes_30_rep_{rep_number:03d}.mp4'\n",
    "            success = self.keyframe_processor.extract_key_frames_and_downsample(\n",
    "                str(repvids_filename), str(repkeys_filename)\n",
    "            )\n",
    "            if success:\n",
    "                print(f\"âœ… Processed rep {rep_number} to RepKeys\")\n",
    "            else:\n",
    "                print(f\"âŒ Failed to process rep {rep_number} to RepKeys\")\n",
    "                \n",
    "        threading.Thread(target=process_to_repkeys, daemon=True).start()\n",
    "        \n",
    "    def run(self, camera_index: int = 0):\n",
    "        \"\"\"Run the integrated detection system.\"\"\"\n",
    "        cap = cv2.VideoCapture(camera_index)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open camera {camera_index}\")\n",
    "            return\n",
    "            \n",
    "        print(\"ðŸš€ Integrated Pushup Detection System Started\")\n",
    "        print(\"ðŸ“¹ Live detection + ðŸ”„ Auto-processing + ðŸ¤– Real-time CNN classification\")\n",
    "        print(\"ðŸ” Automatic bilateral landmark detection (left/right)\")\n",
    "        print(\"Press 'q' to quit\")\n",
    "        \n",
    "        try:\n",
    "            with self.mp_pose.Pose(\n",
    "                min_detection_confidence=self.config.min_detection_confidence,\n",
    "                min_tracking_confidence=self.config.min_tracking_confidence\n",
    "            ) as pose:\n",
    "                \n",
    "                while cap.isOpened():\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                        \n",
    "                    # Process with MediaPipe\n",
    "                    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    image.flags.writeable = False\n",
    "                    results = pose.process(image)\n",
    "                    \n",
    "                    image.flags.writeable = True\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Add frame to CNN classifier for real-time feedback\n",
    "                    self.cnn_classifier.add_frame(image)\n",
    "                    \n",
    "                    # Handle frame buffering for rep recording\n",
    "                    if not self.is_first_rep:\n",
    "                        self.frame_buffer.append(image.copy())\n",
    "                        \n",
    "                    # Extract landmarks and process pushup detection\n",
    "                    landmark_dict = self.extract_landmarks(results)\n",
    "                    label = \"None\"\n",
    "                    side_used = \"None\"\n",
    "                    \n",
    "                    if landmark_dict and self.is_pose_valid(results, landmark_dict, image.shape[0]):\n",
    "                        elbow_angle = AngleCalculator.calculate_angle(\n",
    "                            landmark_dict['shoulder'], \n",
    "                            landmark_dict['elbow'], \n",
    "                            landmark_dict['wrist']\n",
    "                        )\n",
    "                        \n",
    "                        # Determine which side is being used for display\n",
    "                        if results.pose_landmarks:\n",
    "                            landmarks = results.pose_landmarks.landmark\n",
    "                            left_vis = (landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility +\n",
    "                                       landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW.value].visibility +\n",
    "                                       landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value].visibility) / 3\n",
    "                            right_vis = (landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].visibility +\n",
    "                                        landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value].visibility +\n",
    "                                        landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].visibility) / 3\n",
    "                            side_used = \"Left\" if left_vis >= right_vis else \"Right\"\n",
    "                        \n",
    "                        # Update detection state\n",
    "                        prev_counter = self.counter\n",
    "                        \n",
    "                        if elbow_angle > self.config.elbow_angle_up_threshold:\n",
    "                            if self.stage == \"down\":\n",
    "                                self.stage = \"up\"\n",
    "                                self.counter += 1\n",
    "                            else:\n",
    "                                self.stage = \"up\"\n",
    "                        elif elbow_angle < self.config.elbow_angle_down_threshold and self.stage == \"up\":\n",
    "                            self.stage = \"down\"\n",
    "                            \n",
    "                        # Handle rep recording\n",
    "                        if self.is_first_rep:\n",
    "                            # First rep handling\n",
    "                            if self.stage == \"up\" and not self.first_stage_up_detected:\n",
    "                                self.first_stage_up_detected = True\n",
    "                                self.first_rep_buffer = [image.copy()]\n",
    "                                print(\"ðŸ”´ First rep: Started collecting frames\")\n",
    "                            elif self.first_stage_up_detected:\n",
    "                                self.first_rep_buffer.append(image.copy())\n",
    "                                \n",
    "                            if self.counter > prev_counter and self.counter == 1:\n",
    "                                self.process_rep_completion(image, self.counter, self.first_rep_buffer)\n",
    "                                self.first_rep_buffer = []\n",
    "                                self.is_first_rep = False\n",
    "                                print(\"ðŸ”„ Switching to normal rep recording\")\n",
    "                                \n",
    "                        elif self.counter > prev_counter:\n",
    "                            # Normal rep handling\n",
    "                            self.process_rep_completion(image, self.counter, self.frame_buffer)\n",
    "                            self.frame_buffer = []\n",
    "                            \n",
    "                        label = \"Pushup\"\n",
    "                        \n",
    "                        # Display angle info with side indicator\n",
    "                        percent = np.interp(elbow_angle, [90, 160], [0, 100])\n",
    "                        cv2.putText(image, f'{side_used} Elbow: {int(elbow_angle)}Â°', (10, 100),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                        cv2.putText(image, f'{int(percent)}%', (10, 140),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                                   \n",
    "                    # Get CNN classification result\n",
    "                    cnn_result, cnn_confidence = self.cnn_classifier.get_result()\n",
    "                    \n",
    "                    # Display CNN feedback\n",
    "                    if \"Correct\" in cnn_result:\n",
    "                        cnn_color = (0, 255, 0)  # Green\n",
    "                    elif \"Incorrect\" in cnn_result:\n",
    "                        cnn_color = (0, 0, 255)  # Red\n",
    "                    else:\n",
    "                        cnn_color = (255, 255, 0)  # Yellow\n",
    "                        \n",
    "                    cv2.putText(image, f\"Form: {cnn_result}\", (10, 170),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, cnn_color, 2)\n",
    "                    cv2.putText(image, f\"Confidence: {cnn_confidence:.1f}%\", (10, 200),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, cnn_color, 2)\n",
    "                               \n",
    "                    # Buffer status\n",
    "                    if self.is_first_rep:\n",
    "                        if self.first_stage_up_detected:\n",
    "                            cv2.putText(image, f'First Rep Buffer: {len(self.first_rep_buffer)}', \n",
    "                                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "                        else:\n",
    "                            cv2.putText(image, 'Waiting for first \"up\" stage...', \n",
    "                                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "                    else:\n",
    "                        cv2.putText(image, f'Buffer: {len(self.frame_buffer)}/{self.config.buffer_size}', \n",
    "                                   (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "                                   \n",
    "                    # Draw pose landmarks\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        image, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=4),\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "                    )\n",
    "                    \n",
    "                    # Status and rep counter\n",
    "                    rep_status = \" (FIRST REP)\" if self.is_first_rep else \"\"\n",
    "                    cv2.putText(image, f'Detection: {label}{rep_status}', (10, 40),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "                               \n",
    "                    # Rep counter\n",
    "                    cv2.rectangle(image, (image.shape[1] - 160, 0), (image.shape[1], 80), (0, 0, 0), -1)\n",
    "                    cv2.putText(image, 'REPS', (image.shape[1] - 150, 30),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "                    cv2.putText(image, str(self.counter), (image.shape[1] - 140, 70),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 4)\n",
    "                               \n",
    "                    # CNN buffer status\n",
    "                    cv2.putText(image, f'CNN Buffer: {len(self.cnn_classifier.frame_buffer)}/30', \n",
    "                               (10, 230), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "                               \n",
    "                    # Side indicator\n",
    "                    if side_used != \"None\":\n",
    "                        cv2.putText(image, f'Using: {side_used} Side', (10, 260),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (128, 255, 128), 1)\n",
    "                               \n",
    "                    cv2.imshow('Integrated Pushup Detection System', image)\n",
    "                    \n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during execution: {e}\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the integrated pushup detection system.\"\"\"\n",
    "    config = IntegratedConfig(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "        elbow_angle_up_threshold=165.0,\n",
    "        elbow_angle_down_threshold=95.0,\n",
    "        target_frames=30\n",
    "    )\n",
    "    \n",
    "    detector = IntegratedPushupDetector(config)\n",
    "    detector.run()\n",
    "\n",
    "\n",
    "# Run the integrated system\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682378d",
   "metadata": {},
   "source": [
    "# ihfivbfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea85c2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CNN model: pushup_classification_latest_improved.h5\n",
      "ðŸš€ Efficient Pushup Detection System\n",
      "ðŸ“¹ Live detection + ðŸ”„ Auto-processing + ðŸ¤– Real-time CNN\n",
      "Press 'q' to quit\n",
      "âœ… Saved rep 1 (173 frames)\n",
      "âœ… Saved rep 2 (33 frames)\n",
      "âœ… Extracted 24 keyframes for rep 2\n",
      "âœ… Saved rep 3 (34 frames)\n",
      "âœ… Extracted 26 keyframes for rep 3\n",
      "âœ… Saved rep 4 (38 frames)\n",
      "âœ… Extracted 23 keyframes for rep 4\n",
      "âœ… Extracted 23 keyframes for rep 1\n",
      "âœ… Saved rep 5 (85 frames)\n",
      "âœ… Saved rep 6 (52 frames)\n",
      "âœ… Extracted 30 keyframes for rep 5\n",
      "âœ… Extracted 28 keyframes for rep 6\n",
      "âœ… Saved rep 7 (56 frames)\n",
      "âœ… Saved rep 8 (37 frames)\n",
      "âœ… Extracted 26 keyframes for rep 7\n",
      "âœ… Extracted 3 keyframes for rep 8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Efficient Pushup Detection and Classification System\n",
    "Streamlined version with threading, async prediction, and reduced redundancy\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import glob\n",
    "from tensorflow.keras.models import load_model\n",
    "from pathlib import Path\n",
    "import queue\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Streamlined configuration.\"\"\"\n",
    "    # Detection thresholds\n",
    "    min_detection_confidence: float = 0.5\n",
    "    min_tracking_confidence: float = 0.5\n",
    "    elbow_angle_up: float = 165.0\n",
    "    elbow_angle_down: float = 95.0\n",
    "    min_visibility: float = 0.7\n",
    "   \n",
    "    # Video settings\n",
    "    fps: float = 20.0\n",
    "    target_frames: int = 30\n",
    "    frame_size: Tuple[int, int] = (112, 112)\n",
    "   \n",
    "    # Directories\n",
    "    repvids_dir: str = \"RepVids\"\n",
    "    repkeys_dir: str = \"RepKeys\"\n",
    "    models_dir: str = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"Pushup Models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PoseAnalyzer:\n",
    "    \"\"\"Handles pose detection and angle calculations.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "       \n",
    "    @staticmethod\n",
    "    def calculate_angle(a: np.ndarray, b: np.ndarray, c: np.ndarray) -> float:\n",
    "        \"\"\"Calculate angle between three points.\"\"\"\n",
    "        radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "        angle = np.abs(radians * 180.0 / np.pi)\n",
    "        return 360 - angle if angle > 180 else angle\n",
    "   \n",
    "    def extract_best_side_landmarks(self, results) -> Optional[Tuple[Dict, str, float]]:\n",
    "        \"\"\"Extract landmarks from the side with better visibility.\"\"\"\n",
    "        if not results.pose_landmarks:\n",
    "            return None\n",
    "           \n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "       \n",
    "        # Define landmark indices for both sides\n",
    "        left_indices = {\n",
    "            'shoulder': self.mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "            'elbow': self.mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            'wrist': self.mp_pose.PoseLandmark.LEFT_WRIST.value,\n",
    "            'hip': self.mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "            'knee': self.mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "            'ankle': self.mp_pose.PoseLandmark.LEFT_ANKLE.value\n",
    "        }\n",
    "       \n",
    "        right_indices = {\n",
    "            'shoulder': self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            'elbow': self.mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            'wrist': self.mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "            'hip': self.mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "            'knee': self.mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            'ankle': self.mp_pose.PoseLandmark.RIGHT_ANKLE.value\n",
    "        }\n",
    "       \n",
    "        # Calculate visibility for both sides\n",
    "        left_vis = np.mean([landmarks[idx].visibility for idx in left_indices.values()])\n",
    "        right_vis = np.mean([landmarks[idx].visibility for idx in right_indices.values()])\n",
    "       \n",
    "        # Choose better side\n",
    "        if left_vis >= right_vis and left_vis > self.config.min_visibility:\n",
    "            indices = left_indices\n",
    "            side = \"Left\"\n",
    "            visibility = left_vis\n",
    "        elif right_vis > self.config.min_visibility:\n",
    "            indices = right_indices\n",
    "            side = \"Right\"\n",
    "            visibility = right_vis\n",
    "        else:\n",
    "            return None\n",
    "           \n",
    "        # Extract coordinates\n",
    "        coords = {}\n",
    "        for joint, idx in indices.items():\n",
    "            coords[joint] = np.array([landmarks[idx].x, landmarks[idx].y])\n",
    "           \n",
    "        return coords, side, visibility\n",
    "   \n",
    "    def get_elbow_angle(self, coords: Dict) -> float:\n",
    "        \"\"\"Calculate elbow angle from coordinates.\"\"\"\n",
    "        return self.calculate_angle(coords['shoulder'], coords['elbow'], coords['wrist'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VideoProcessor:\n",
    "    \"\"\"Handles video recording and keyframe extraction.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.pose_analyzer = PoseAnalyzer(config)\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "       \n",
    "        # Setup directories\n",
    "        base_path = Path.home() / \"Desktop\" / \"Deep Dive AI Summer 2025\"\n",
    "        self.repvids_path = base_path / config.repvids_dir\n",
    "        self.repkeys_path = base_path / config.repkeys_dir\n",
    "        self.repvids_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.repkeys_path.mkdir(parents=True, exist_ok=True)\n",
    "       \n",
    "    def save_rep_video(self, frames: List[np.ndarray], rep_number: int) -> str:\n",
    "        \"\"\"Save repetition video to RepVids.\"\"\"\n",
    "        if not frames:\n",
    "            return None\n",
    "           \n",
    "        filename = self.repvids_path / f'rep_{rep_number:03d}.mp4'\n",
    "        height, width = frames[0].shape[:2]\n",
    "       \n",
    "        out = cv2.VideoWriter(str(filename), self.fourcc, self.config.fps, (width, height))\n",
    "        for frame in frames:\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "       \n",
    "        print(f\"âœ… Saved rep {rep_number} ({len(frames)} frames)\")\n",
    "        return str(filename)\n",
    "   \n",
    "    def extract_keyframes(self, video_path: str, rep_number: int) -> bool:\n",
    "        \"\"\"Extract and downsample to 30 keyframes based on elbow angles.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "       \n",
    "        with self.pose_analyzer.mp_pose.Pose(\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.7\n",
    "        ) as pose:\n",
    "           \n",
    "            frames = []\n",
    "            angles = []\n",
    "           \n",
    "            # Extract all frames and angles\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                   \n",
    "                frames.append(frame)\n",
    "               \n",
    "                # Get elbow angle\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = pose.process(image_rgb)\n",
    "               \n",
    "                pose_data = self.pose_analyzer.extract_best_side_landmarks(results)\n",
    "                if pose_data:\n",
    "                    coords, _, _ = pose_data\n",
    "                    angle = self.pose_analyzer.get_elbow_angle(coords)\n",
    "                    angles.append(angle)\n",
    "                else:\n",
    "                    angles.append(180)  # Default angle if no pose detected\n",
    "                   \n",
    "        cap.release()\n",
    "       \n",
    "        if len(frames) < 3:\n",
    "            return False\n",
    "           \n",
    "        # Find key points\n",
    "        min_angle_idx = np.argmin(angles)\n",
    "       \n",
    "        # Find start and end based on angle peaks\n",
    "        start_idx = 0\n",
    "        end_idx = len(frames) - 1\n",
    "       \n",
    "        # Look for angle peaks before and after minimum\n",
    "        for i in range(min_angle_idx - 1, -1, -1):\n",
    "            if angles[i] > 160:  # High angle indicating \"up\" position\n",
    "                start_idx = i\n",
    "                break\n",
    "               \n",
    "        for i in range(min_angle_idx + 1, len(angles)):\n",
    "            if angles[i] > 160:  # High angle indicating \"up\" position\n",
    "                end_idx = i\n",
    "                break\n",
    "               \n",
    "        # Extract relevant frames\n",
    "        key_frames = frames[start_idx:end_idx + 1]\n",
    "       \n",
    "        # Downsample to target frames\n",
    "        if len(key_frames) > self.config.target_frames:\n",
    "            indices = np.linspace(0, len(key_frames) - 1, self.config.target_frames, dtype=int)\n",
    "            # Ensure minimum angle frame is included\n",
    "            min_frame_relative = min_angle_idx - start_idx\n",
    "            if min_frame_relative not in indices:\n",
    "                closest_idx = np.argmin(np.abs(indices - min_frame_relative))\n",
    "                indices[closest_idx] = min_frame_relative\n",
    "                indices = np.sort(indices)\n",
    "            key_frames = [key_frames[i] for i in indices]\n",
    "           \n",
    "        # Save keyframes video\n",
    "        if key_frames:\n",
    "            output_path = self.repkeys_path / f'keyframes_30_rep_{rep_number:03d}.mp4'\n",
    "            height, width = key_frames[0].shape[:2]\n",
    "           \n",
    "            out = cv2.VideoWriter(str(output_path), self.fourcc, 10.0, (width, height))\n",
    "            for frame in key_frames:\n",
    "                out.write(frame)\n",
    "            out.release()\n",
    "           \n",
    "            print(f\"âœ… Extracted {len(key_frames)} keyframes for rep {rep_number}\")\n",
    "            return True\n",
    "           \n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNNClassifier:\n",
    "    \"\"\"Efficient CNN classifier with async prediction.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.frame_buffer = deque(maxlen=config.target_frames)\n",
    "        self.prediction_queue = queue.Queue(maxsize=5)\n",
    "        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n",
    "        self.current_prediction = \"Initializing...\"\n",
    "        self.current_confidence = 0.0\n",
    "        self.load_model()\n",
    "       \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the latest CNN model.\"\"\"\n",
    "        os.makedirs(self.config.models_dir, exist_ok=True)\n",
    "        model_files = glob.glob(os.path.join(self.config.models_dir, 'pushup_classification*.h5'))\n",
    "       \n",
    "        if model_files:\n",
    "            latest_model = max(model_files, key=os.path.getmtime)\n",
    "            self.model = load_model(latest_model)\n",
    "            print(f\"Loaded CNN model: {os.path.basename(latest_model)}\")\n",
    "        else:\n",
    "            print(\"No CNN model found - classification disabled\")\n",
    "           \n",
    "    def preprocess_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess frame for CNN.\"\"\"\n",
    "        processed = cv2.resize(frame, self.config.frame_size)\n",
    "        return (processed / 255.0).astype('float32')\n",
    "       \n",
    "    def predict_async(self, frames: List[np.ndarray]):\n",
    "        \"\"\"Async prediction function.\"\"\"\n",
    "        try:\n",
    "            if self.model is None:\n",
    "                return \"No Model\", 0.0\n",
    "               \n",
    "            # Prepare sequence\n",
    "            sequence = np.array([self.preprocess_frame(f) for f in frames])\n",
    "            sequence = np.expand_dims(sequence, axis=0)\n",
    "           \n",
    "            # Predict\n",
    "            prediction = self.model.predict(sequence, verbose=0)\n",
    "            predicted_class = np.argmax(prediction)\n",
    "            confidence = np.max(prediction) * 100\n",
    "           \n",
    "            labels = ['Incorrect Form', 'Correct Form']\n",
    "            return labels[predicted_class], confidence\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            return \"Error\", 0.0\n",
    "           \n",
    "    def add_frame(self, frame: np.ndarray):\n",
    "        \"\"\"Add frame and trigger prediction if buffer is full.\"\"\"\n",
    "        self.frame_buffer.append(frame.copy())\n",
    "       \n",
    "        if len(self.frame_buffer) == self.config.target_frames:\n",
    "            # Submit async prediction\n",
    "            future = self.executor.submit(self.predict_async, list(self.frame_buffer))\n",
    "           \n",
    "            # Non-blocking result retrieval\n",
    "            def update_result():\n",
    "                try:\n",
    "                    result, confidence = future.result(timeout=0.1)\n",
    "                    self.current_prediction = result\n",
    "                    self.current_confidence = confidence\n",
    "                except concurrent.futures.TimeoutError:\n",
    "                    pass  # Keep previous result\n",
    "                except Exception as e:\n",
    "                    print(f\"Prediction update error: {e}\")\n",
    "                   \n",
    "            threading.Thread(target=update_result, daemon=True).start()\n",
    "           \n",
    "    def get_result(self) -> Tuple[str, float]:\n",
    "        \"\"\"Get current prediction result.\"\"\"\n",
    "        return self.current_prediction, self.current_confidence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EfficientPushupDetector:\n",
    "    \"\"\"Main efficient pushup detection system.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config = None):\n",
    "        self.config = config or Config()\n",
    "       \n",
    "        # Initialize components\n",
    "        self.pose_analyzer = PoseAnalyzer(self.config)\n",
    "        self.video_processor = VideoProcessor(self.config)\n",
    "        self.cnn_classifier = CNNClassifier(self.config)\n",
    "       \n",
    "        # MediaPipe setup\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "       \n",
    "        # Detection state\n",
    "        self.counter = 0\n",
    "        self.stage = None\n",
    "        self.frame_buffer = []\n",
    "        self.processing_executor = concurrent.futures.ThreadPoolExecutor(max_workers=3)\n",
    "       \n",
    "    def process_completed_rep(self, frames: List[np.ndarray], rep_number: int):\n",
    "        \"\"\"Process completed rep in background thread.\"\"\"\n",
    "        def process():\n",
    "            # Save to RepVids\n",
    "            video_path = self.video_processor.save_rep_video(frames, rep_number)\n",
    "            if video_path:\n",
    "                # Extract keyframes to RepKeys\n",
    "                self.video_processor.extract_keyframes(video_path, rep_number)\n",
    "               \n",
    "        self.processing_executor.submit(process)\n",
    "       \n",
    "    def run(self, camera_index: int = 0):\n",
    "        \"\"\"Run the efficient detection system.\"\"\"\n",
    "        cap = cv2.VideoCapture(camera_index)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "       \n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open camera {camera_index}\")\n",
    "            return\n",
    "           \n",
    "        print(\"ðŸš€ Efficient Pushup Detection System\")\n",
    "        print(\"ðŸ“¹ Live detection + ðŸ”„ Auto-processing + ðŸ¤– Real-time CNN\")\n",
    "        print(\"Press 'q' to quit\")\n",
    "       \n",
    "        try:\n",
    "            with self.mp_pose.Pose(\n",
    "                min_detection_confidence=self.config.min_detection_confidence,\n",
    "                min_tracking_confidence=self.config.min_tracking_confidence\n",
    "            ) as pose:\n",
    "               \n",
    "                while cap.isOpened():\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                       \n",
    "                    # Process pose\n",
    "                    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    results = pose.process(image_rgb)\n",
    "                   \n",
    "                    # Add frame to CNN classifier\n",
    "                    self.cnn_classifier.add_frame(frame)\n",
    "                   \n",
    "                    # Extract pose data\n",
    "                    pose_data = self.pose_analyzer.extract_best_side_landmarks(results)\n",
    "                   \n",
    "                    if pose_data:\n",
    "                        coords, side, visibility = pose_data\n",
    "                        elbow_angle = self.pose_analyzer.get_elbow_angle(coords)\n",
    "                       \n",
    "                        # Update detection state\n",
    "                        prev_counter = self.counter\n",
    "                       \n",
    "                        if elbow_angle > self.config.elbow_angle_up:\n",
    "                            if self.stage == \"down\":\n",
    "                                self.stage = \"up\"\n",
    "                                self.counter += 1\n",
    "                            else:\n",
    "                                self.stage = \"up\"\n",
    "                        elif elbow_angle < self.config.elbow_angle_down and self.stage == \"up\":\n",
    "                            self.stage = \"down\"\n",
    "                           \n",
    "                        # Handle frame buffering and rep completion\n",
    "                        self.frame_buffer.append(frame.copy())\n",
    "                       \n",
    "                        if self.counter > prev_counter:\n",
    "                            # Rep completed - process in background\n",
    "                            self.process_completed_rep(self.frame_buffer.copy(), self.counter)\n",
    "                            self.frame_buffer = []\n",
    "                           \n",
    "                        # Display info\n",
    "                        percent = np.interp(elbow_angle, [90, 160], [0, 100])\n",
    "                        cv2.putText(frame, f'{side} Elbow: {int(elbow_angle)}Â°', (10, 60),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                        cv2.putText(frame, f'{int(percent)}%', (10, 90),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "                                   \n",
    "                    # Get and display CNN result\n",
    "                    cnn_result, cnn_confidence = self.cnn_classifier.get_result()\n",
    "                    cnn_color = (0, 255, 0) if \"Correct\" in cnn_result else (0, 0, 255)\n",
    "                   \n",
    "                    cv2.putText(frame, f\"Form: {cnn_result}\", (10, 120),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, cnn_color, 2)\n",
    "                    cv2.putText(frame, f\"Confidence: {cnn_confidence:.1f}%\", (10, 150),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, cnn_color, 2)\n",
    "                               \n",
    "                    # Draw pose landmarks\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=3),\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "                    )\n",
    "                   \n",
    "                    # Rep counter\n",
    "                    cv2.rectangle(frame, (frame.shape[1] - 120, 0), (frame.shape[1], 60), (0, 0, 0), -1)\n",
    "                    cv2.putText(frame, 'REPS', (frame.shape[1] - 110, 25),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                    cv2.putText(frame, str(self.counter), (frame.shape[1] - 100, 50),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 3)\n",
    "                               \n",
    "                    # Buffer status\n",
    "                    cv2.putText(frame, f'Buffer: {len(self.frame_buffer)}', (10, 30),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "                    cv2.putText(frame, f'CNN: {len(self.cnn_classifier.frame_buffer)}/30', (10, 180),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "                               \n",
    "                    cv2.imshow('Efficient Pushup Detection', frame)\n",
    "                   \n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                       \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            self.processing_executor.shutdown(wait=False)\n",
    "            self.cnn_classifier.executor.shutdown(wait=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the efficient pushup detection system.\"\"\"\n",
    "    config = Config(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "        elbow_angle_up=165.0,\n",
    "        elbow_angle_down=95.0,\n",
    "        target_frames=30\n",
    "    )\n",
    "   \n",
    "    detector = EfficientPushupDetector(config)\n",
    "    detector.run()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsizing goes to any number under 30 which we have code to fix\n",
    "\n",
    "# model doesnt have text to show its running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6b570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CNN model: pushup_classification_latest_improved.h5\n",
      "ðŸš€ Efficient Pushup Detection System\n",
      "ðŸ“¹ Live detection + ðŸ”„ Auto-processing + ðŸ¤– Real-time CNN\n",
      "Press 'q' to quit\n",
      "ðŸ”´ First rep: Started collecting frames (stage = 'up')\n",
      "ðŸ”„ Switching to normal rep recording mode\n",
      "âœ… Saved rep 1 (71 frames)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted 30 keyframes for rep 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Efficient Pushup Detection and Classification System\n",
    "Streamlined version with threading, async prediction, and reduced redundancy\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import glob\n",
    "from tensorflow.keras.models import load_model\n",
    "from pathlib import Path\n",
    "import queue\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Streamlined configuration.\"\"\"\n",
    "    # Detection thresholds\n",
    "    min_detection_confidence: float = 0.5\n",
    "    min_tracking_confidence: float = 0.5\n",
    "    elbow_angle_up: float = 165.0\n",
    "    elbow_angle_down: float = 95.0\n",
    "    min_visibility: float = 0.7\n",
    "   \n",
    "    # Video settings\n",
    "    fps: float = 20.0\n",
    "    target_frames: int = 30\n",
    "    frame_size: Tuple[int, int] = (112, 112)\n",
    "   \n",
    "    # Directories\n",
    "    repvids_dir: str = \"RepVids\"\n",
    "    repkeys_dir: str = \"RepKeys\"\n",
    "    models_dir: str = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"Pushup Models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PoseAnalyzer:\n",
    "    \"\"\"Handles pose detection and angle calculations.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "       \n",
    "    @staticmethod\n",
    "    def calculate_angle(a: np.ndarray, b: np.ndarray, c: np.ndarray) -> float:\n",
    "        \"\"\"Calculate angle between three points.\"\"\"\n",
    "        radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "        angle = np.abs(radians * 180.0 / np.pi)\n",
    "        return 360 - angle if angle > 180 else angle\n",
    "   \n",
    "    def extract_best_side_landmarks(self, results) -> Optional[Tuple[Dict, str, float]]:\n",
    "        \"\"\"Extract landmarks from the side with better visibility.\"\"\"\n",
    "        if not results.pose_landmarks:\n",
    "            return None\n",
    "           \n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "       \n",
    "        # Define landmark indices for both sides\n",
    "        left_indices = {\n",
    "            'shoulder': self.mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "            'elbow': self.mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            'wrist': self.mp_pose.PoseLandmark.LEFT_WRIST.value,\n",
    "            'hip': self.mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "            'knee': self.mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "            'ankle': self.mp_pose.PoseLandmark.LEFT_ANKLE.value\n",
    "        }\n",
    "       \n",
    "        right_indices = {\n",
    "            'shoulder': self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            'elbow': self.mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            'wrist': self.mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "            'hip': self.mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "            'knee': self.mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            'ankle': self.mp_pose.PoseLandmark.RIGHT_ANKLE.value\n",
    "        }\n",
    "       \n",
    "        # Calculate visibility for both sides\n",
    "        left_vis = np.mean([landmarks[idx].visibility for idx in left_indices.values()])\n",
    "        right_vis = np.mean([landmarks[idx].visibility for idx in right_indices.values()])\n",
    "       \n",
    "        # Choose better side\n",
    "        if left_vis >= right_vis and left_vis > self.config.min_visibility:\n",
    "            indices = left_indices\n",
    "            side = \"Left\"\n",
    "            visibility = left_vis\n",
    "        elif right_vis > self.config.min_visibility:\n",
    "            indices = right_indices\n",
    "            side = \"Right\"\n",
    "            visibility = right_vis\n",
    "        else:\n",
    "            return None\n",
    "           \n",
    "        # Extract coordinates\n",
    "        coords = {}\n",
    "        for joint, idx in indices.items():\n",
    "            coords[joint] = np.array([landmarks[idx].x, landmarks[idx].y])\n",
    "           \n",
    "        return coords, side, visibility\n",
    "   \n",
    "    def get_elbow_angle(self, coords: Dict) -> float:\n",
    "        \"\"\"Calculate elbow angle from coordinates.\"\"\"\n",
    "        return self.calculate_angle(coords['shoulder'], coords['elbow'], coords['wrist'])\n",
    "\n",
    "class VideoProcessor:\n",
    "    \"\"\"Handles video recording and keyframe extraction.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.pose_analyzer = PoseAnalyzer(config)\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "       \n",
    "        # Setup directories\n",
    "        base_path = Path.home() / \"Desktop\" / \"Deep Dive AI Summer 2025\"\n",
    "        self.repvids_path = base_path / config.repvids_dir\n",
    "        self.repkeys_path = base_path / config.repkeys_dir\n",
    "        self.repvids_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.repkeys_path.mkdir(parents=True, exist_ok=True)\n",
    "       \n",
    "    def save_rep_video(self, frames: List[np.ndarray], rep_number: int) -> str:\n",
    "        \"\"\"Save repetition video to RepVids.\"\"\"\n",
    "        if not frames:\n",
    "            return None\n",
    "           \n",
    "        filename = self.repvids_path / f'rep_{rep_number:03d}.mp4'\n",
    "        height, width = frames[0].shape[:2]\n",
    "       \n",
    "        out = cv2.VideoWriter(str(filename), self.fourcc, self.config.fps, (width, height))\n",
    "        for frame in frames:\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "       \n",
    "        print(f\"âœ… Saved rep {rep_number} ({len(frames)} frames)\")\n",
    "        return str(filename)\n",
    "   \n",
    "    def extract_keyframes(self, video_path: str, rep_number: int) -> bool:\n",
    "        \"\"\"Extract and downsample to exactly 30 keyframes based on elbow angles.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "        with self.pose_analyzer.mp_pose.Pose(\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.7\n",
    "        ) as pose:\n",
    "        \n",
    "            frames = []\n",
    "            angles = []\n",
    "        \n",
    "            # Extract all frames and angles\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                frames.append(frame)\n",
    "            \n",
    "                # Get elbow angle\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = pose.process(image_rgb)\n",
    "            \n",
    "                pose_data = self.pose_analyzer.extract_best_side_landmarks(results)\n",
    "                if pose_data:\n",
    "                    coords, _, _ = pose_data\n",
    "                    angle = self.pose_analyzer.get_elbow_angle(coords)\n",
    "                    angles.append(angle)\n",
    "                else:\n",
    "                    angles.append(180)  # Default angle if no pose detected\n",
    "                \n",
    "        cap.release()\n",
    "    \n",
    "        if len(frames) < 3:\n",
    "            return False\n",
    "        \n",
    "        # Find key points\n",
    "        min_angle_idx = np.argmin(angles)\n",
    "    \n",
    "        # Find start and end based on angle peaks\n",
    "        start_idx = 0\n",
    "        end_idx = len(frames) - 1\n",
    "    \n",
    "        # Look for angle peaks before and after minimum\n",
    "        for i in range(min_angle_idx - 1, -1, -1):\n",
    "            if angles[i] > 160:  # High angle indicating \"up\" position\n",
    "                start_idx = i\n",
    "                break\n",
    "            \n",
    "        for i in range(min_angle_idx + 1, len(angles)):\n",
    "            if angles[i] > 160:  # High angle indicating \"up\" position\n",
    "                end_idx = i\n",
    "                break\n",
    "            \n",
    "        # Extract relevant frames\n",
    "        key_frames = frames[start_idx:end_idx + 1]\n",
    "    \n",
    "        # ALWAYS ensure exactly 30 frames\n",
    "        if len(key_frames) == self.config.target_frames:\n",
    "            # Perfect - use as is\n",
    "            final_frames = key_frames\n",
    "        elif len(key_frames) > self.config.target_frames:\n",
    "            # Downsample to exactly 30\n",
    "            indices = np.linspace(0, len(key_frames) - 1, self.config.target_frames, dtype=int)\n",
    "            # Ensure minimum angle frame is included\n",
    "            min_frame_relative = min_angle_idx - start_idx\n",
    "            if min_frame_relative not in indices:\n",
    "                closest_idx = np.argmin(np.abs(indices - min_frame_relative))\n",
    "                indices[closest_idx] = min_frame_relative\n",
    "                indices = np.sort(indices)\n",
    "            final_frames = [key_frames[i] for i in indices]\n",
    "        else:\n",
    "            # Upsample to exactly 30 frames by repeating/interpolating\n",
    "            if len(key_frames) < self.config.target_frames:\n",
    "                # Create indices that will repeat frames to reach target\n",
    "                indices = np.linspace(0, len(key_frames) - 1, self.config.target_frames)\n",
    "                indices = np.round(indices).astype(int)\n",
    "                \n",
    "                # Ensure minimum angle frame is still included\n",
    "                min_frame_relative = min_angle_idx - start_idx\n",
    "                if min_frame_relative < len(key_frames):\n",
    "                    # Find where min frame should be in the upsampled sequence\n",
    "                    target_pos = int((min_frame_relative / (len(key_frames) - 1)) * (self.config.target_frames - 1))\n",
    "                    indices[target_pos] = min_frame_relative\n",
    "                \n",
    "                final_frames = [key_frames[i] for i in indices]\n",
    "    \n",
    "        # Save keyframes video - now guaranteed to be exactly 30 frames\n",
    "        if final_frames:\n",
    "            output_path = self.repkeys_path / f'keyframes_30_rep_{rep_number:03d}.mp4'\n",
    "            height, width = final_frames[0].shape[:2]\n",
    "        \n",
    "            out = cv2.VideoWriter(str(output_path), self.fourcc, 10.0, (width, height))\n",
    "            for frame in final_frames:\n",
    "                out.write(frame)\n",
    "            out.release()\n",
    "        \n",
    "            print(f\"âœ… Extracted {len(final_frames)} keyframes for rep {rep_number}\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "class CNNClassifier:\n",
    "    \"\"\"Efficient CNN classifier with async prediction.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.frame_buffer = deque(maxlen=config.target_frames)\n",
    "        self.prediction_queue = queue.Queue(maxsize=5)\n",
    "        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n",
    "        self.current_prediction = \"Initializing...\"\n",
    "        self.current_confidence = 0.0\n",
    "        self.load_model()\n",
    "       \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the latest CNN model.\"\"\"\n",
    "        os.makedirs(self.config.models_dir, exist_ok=True)\n",
    "        model_files = glob.glob(os.path.join(self.config.models_dir, 'pushup_classification*.h5'))\n",
    "       \n",
    "        if model_files:\n",
    "            latest_model = max(model_files, key=os.path.getmtime)\n",
    "            self.model = load_model(latest_model)\n",
    "            print(f\"Loaded CNN model: {os.path.basename(latest_model)}\")\n",
    "        else:\n",
    "            print(\"No CNN model found - classification disabled\")\n",
    "           \n",
    "    def preprocess_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess frame for CNN.\"\"\"\n",
    "        processed = cv2.resize(frame, self.config.frame_size)\n",
    "        return (processed / 255.0).astype('float32')\n",
    "       \n",
    "    def predict_async(self, frames: List[np.ndarray]):\n",
    "        \"\"\"Async prediction function.\"\"\"\n",
    "        try:\n",
    "            if self.model is None:\n",
    "                return \"No Model\", 0.0\n",
    "               \n",
    "            # Prepare sequence\n",
    "            sequence = np.array([self.preprocess_frame(f) for f in frames])\n",
    "            sequence = np.expand_dims(sequence, axis=0)\n",
    "           \n",
    "            # Predict\n",
    "            prediction = self.model.predict(sequence, verbose=0)\n",
    "            predicted_class = np.argmax(prediction)\n",
    "            confidence = np.max(prediction) * 100\n",
    "           \n",
    "            labels = ['Incorrect Form', 'Correct Form']\n",
    "            return labels[predicted_class], confidence\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            return \"Error\", 0.0\n",
    "           \n",
    "    def add_frame(self, frame: np.ndarray):\n",
    "        \"\"\"Add frame and trigger prediction if buffer is full.\"\"\"\n",
    "        self.frame_buffer.append(frame.copy())\n",
    "       \n",
    "        if len(self.frame_buffer) == self.config.target_frames:\n",
    "            # Submit async prediction\n",
    "            future = self.executor.submit(self.predict_async, list(self.frame_buffer))\n",
    "           \n",
    "            # Non-blocking result retrieval\n",
    "            def update_result():\n",
    "                try:\n",
    "                    result, confidence = future.result(timeout=0.1)\n",
    "                    self.current_prediction = result\n",
    "                    self.current_confidence = confidence\n",
    "                except concurrent.futures.TimeoutError:\n",
    "                    pass  # Keep previous result\n",
    "                except Exception as e:\n",
    "                    print(f\"Prediction update error: {e}\")\n",
    "                   \n",
    "            threading.Thread(target=update_result, daemon=True).start()\n",
    "           \n",
    "    def get_result(self) -> Tuple[str, float]:\n",
    "        \"\"\"Get current prediction result.\"\"\"\n",
    "        return self.current_prediction, self.current_confidence\n",
    "\n",
    "class EfficientPushupDetector:\n",
    "    \"\"\"Main efficient pushup detection system. \n",
    "    WITH RUN FUNCTION.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config = None):\n",
    "        self.config = config or Config()\n",
    "       \n",
    "        # Initialize components\n",
    "        self.pose_analyzer = PoseAnalyzer(self.config)\n",
    "        self.video_processor = VideoProcessor(self.config)\n",
    "        self.cnn_classifier = CNNClassifier(self.config)\n",
    "       \n",
    "        # MediaPipe setup\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "       \n",
    "        # Detection state\n",
    "        self.counter = 0\n",
    "        self.stage = None\n",
    "        self.frame_buffer = []\n",
    "        self.first_rep_buffer = []  # Add this\n",
    "        self.first_stage_up_detected = False  # Add this\n",
    "        self.is_first_rep = True  # Add this\n",
    "        self.processing_executor = concurrent.futures.ThreadPoolExecutor(max_workers=3)\n",
    "       \n",
    "    def process_completed_rep(self, frames: List[np.ndarray], rep_number: int):\n",
    "        \"\"\"Process completed rep in background thread.\"\"\"\n",
    "        def process():\n",
    "            # Save to RepVids\n",
    "            video_path = self.video_processor.save_rep_video(frames, rep_number)\n",
    "            if video_path:\n",
    "                # Extract keyframes to RepKeys\n",
    "                self.video_processor.extract_keyframes(video_path, rep_number)\n",
    "               \n",
    "        self.processing_executor.submit(process)\n",
    "       \n",
    "    def run(self, camera_index: int = 0):\n",
    "        \"\"\"Run the efficient detection system.\"\"\"\n",
    "        cap = cv2.VideoCapture(camera_index)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "       \n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open camera {camera_index}\")\n",
    "            return\n",
    "           \n",
    "        print(\"ðŸš€ Efficient Pushup Detection System\")\n",
    "        print(\"ðŸ“¹ Live detection + ðŸ”„ Auto-processing + ðŸ¤– Real-time CNN\")\n",
    "        print(\"Press 'q' to quit\")\n",
    "       \n",
    "        try:\n",
    "            with self.mp_pose.Pose(\n",
    "                min_detection_confidence=self.config.min_detection_confidence,\n",
    "                min_tracking_confidence=self.config.min_tracking_confidence\n",
    "            ) as pose:\n",
    "               \n",
    "                while cap.isOpened():\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                       \n",
    "                    # Process pose\n",
    "                    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    results = pose.process(image_rgb)\n",
    "                   \n",
    "                    # Add frame to CNN classifier\n",
    "                    self.cnn_classifier.add_frame(frame)\n",
    "                   \n",
    "                    # Extract pose data\n",
    "                    pose_data = self.pose_analyzer.extract_best_side_landmarks(results)\n",
    "                   \n",
    "                    if pose_data:\n",
    "                        coords, side, visibility = pose_data\n",
    "                        elbow_angle = self.pose_analyzer.get_elbow_angle(coords)\n",
    "                       \n",
    "                        # Update detection state\n",
    "                        prev_counter = self.counter\n",
    "                       \n",
    "                        if elbow_angle > self.config.elbow_angle_up:\n",
    "                            if self.stage == \"down\":\n",
    "                                self.stage = \"up\"\n",
    "                                self.counter += 1\n",
    "                            else:\n",
    "                                self.stage = \"up\"\n",
    "                        elif elbow_angle < self.config.elbow_angle_down and self.stage == \"up\":\n",
    "                            self.stage = \"down\"\n",
    "                           \n",
    "                        # Handle frame buffering and rep completion\n",
    "                        self.frame_buffer.append(frame.copy())\n",
    "\n",
    "                        if self.is_first_rep:\n",
    "                            # Start collecting frames when stage first becomes \"up\"\n",
    "                            if self.stage == \"up\" and not self.first_stage_up_detected:\n",
    "                                self.first_stage_up_detected = True\n",
    "                                self.first_rep_buffer = [frame.copy()]  # Start with current frame\n",
    "                                print(\"ðŸ”´ First rep: Started collecting frames (stage = 'up')\")\n",
    "                            \n",
    "                            # Continue collecting frames if we've started\n",
    "                            elif self.first_stage_up_detected:\n",
    "                                self.first_rep_buffer.append(frame.copy())\n",
    "                            \n",
    "                            # Save first rep when counter increases from 0 to 1\n",
    "                            if self.counter > prev_counter and self.counter == 1:\n",
    "                                # Rep completed - process in background (same pattern as normal reps)\n",
    "                                self.process_completed_rep(self.first_rep_buffer.copy(), self.counter)\n",
    "                                \n",
    "                                # Reset first rep variables\n",
    "                                self.first_rep_buffer = []\n",
    "                                self.is_first_rep = False\n",
    "                                print(\"ðŸ”„ Switching to normal rep recording mode\")\n",
    "\n",
    "                        elif self.counter > prev_counter:\n",
    "                            # Rep completed - process in background\n",
    "                            self.process_completed_rep(self.frame_buffer.copy(), self.counter)\n",
    "                            self.frame_buffer = []\n",
    "        \n",
    "                    # Get and display CNN result\n",
    "                    cnn_result, cnn_confidence = self.cnn_classifier.get_result()\n",
    "                    cnn_color = (0, 255, 0) if \"Correct\" in cnn_result else (0, 0, 255)\n",
    "                   \n",
    "                    cv2.putText(frame, f\"Form: {cnn_result}\", (10, 120),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, cnn_color, 2)\n",
    "                    cv2.putText(frame, f\"Confidence: {cnn_confidence:.1f}%\", (10, 150),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, cnn_color, 2)\n",
    "                               \n",
    "                    # Draw pose landmarks\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=3),\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "                    )\n",
    "                   \n",
    "                    # Rep counter\n",
    "                    cv2.rectangle(frame, (frame.shape[1] - 120, 0), (frame.shape[1], 60), (0, 0, 0), -1)\n",
    "                    cv2.putText(frame, 'REPS', (frame.shape[1] - 110, 25),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                    cv2.putText(frame, str(self.counter), (frame.shape[1] - 100, 50),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 3)\n",
    "                               \n",
    "                    # Buffer status\n",
    "                    cv2.putText(frame, f'Buffer: {len(self.frame_buffer)}', (10, 30),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "                    cv2.putText(frame, f'CNN: {len(self.cnn_classifier.frame_buffer)}/30', (10, 180),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "                               \n",
    "                    cv2.imshow('Efficient Pushup Detection', frame)\n",
    "                   \n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                       \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            self.processing_executor.shutdown(wait=False)\n",
    "            self.cnn_classifier.executor.shutdown(wait=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the efficient pushup detection system.\"\"\"\n",
    "    config = Config(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "        elbow_angle_up=165.0,\n",
    "        elbow_angle_down=95.0,\n",
    "        target_frames=30\n",
    "    )\n",
    "   \n",
    "    detector = EfficientPushupDetector(config)\n",
    "    detector.run()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# can run without clearing files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba414ffc",
   "metadata": {},
   "source": [
    "# *IMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738f680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded CNN model: pushup_classification_latest_improved.h5\n",
      "ðŸš€ Efficient Pushup Detection System\n",
      "ðŸ“¹ Live detection + ðŸ”„ Auto-processing + ðŸ¤– CNN Classification\n",
      "Press 'q' to quit\n",
      "ðŸ”´ First rep: Started collecting frames (stage = 'up')\n",
      "ðŸ”„ Switching to normal rep recording mode\n",
      "âœ… Saved rep 1 (56 frames)\n",
      "âœ… Saved rep 2 (48 frames)\n",
      "âœ… Extracted 30 keyframes for rep 1\n",
      "ðŸ” Rep 1: Incorrect Form (Confidence: 100.0%)\n",
      "âœ… Extracted 30 keyframes for rep 2\n",
      "ðŸ” Rep 2: Incorrect Form (Confidence: 100.0%)\n",
      "âœ… Saved rep 3 (64 frames)\n",
      "âœ… Saved rep 4 (60 frames)\n",
      "âœ… Extracted 30 keyframes for rep 3\n",
      "ðŸ” Rep 3: Incorrect Form (Confidence: 100.0%)\n",
      "âœ… Saved rep 5 (46 frames)\n",
      "âœ… Extracted 30 keyframes for rep 4\n",
      "ðŸ” Rep 4: Incorrect Form (Confidence: 100.0%)\n",
      "âœ… Extracted 30 keyframes for rep 5\n",
      "ðŸ” Rep 5: Incorrect Form (Confidence: 100.0%)\n",
      "\n",
      "ðŸ” Final classification of all reps:\n",
      "\n",
      "ðŸ¤– CNN Classification Results:\n",
      "==================================================\n",
      "ðŸ” Rep 1: Incorrect Form (Confidence: 100.0%)\n",
      "ðŸ” Rep 2: Incorrect Form (Confidence: 100.0%)\n",
      "ðŸ” Rep 3: Incorrect Form (Confidence: 100.0%)\n",
      "ðŸ” Rep 4: Incorrect Form (Confidence: 100.0%)\n",
      "ðŸ” Rep 5: Incorrect Form (Confidence: 100.0%)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Efficient Pushup Detection and Classification System\n",
    "Streamlined version with threading, async prediction, and reduced redundancy\n",
    "\"\"\"\n",
    "\n",
    "# CLEAR REPVIDS AND REPKEYS FILES BEFORE RUNNING\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import glob\n",
    "from tensorflow.keras.models import load_model\n",
    "from pathlib import Path\n",
    "import queue\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Streamlined configuration.\"\"\"\n",
    "    # Detection thresholds\n",
    "    min_detection_confidence: float = 0.5\n",
    "    min_tracking_confidence: float = 0.5\n",
    "    elbow_angle_up: float = 165.0\n",
    "    elbow_angle_down: float = 95.0\n",
    "    min_visibility: float = 0.7\n",
    "   \n",
    "    # Video settings\n",
    "    fps: float = 20.0\n",
    "    target_frames: int = 30\n",
    "    frame_size: Tuple[int, int] = (112, 112)\n",
    "   \n",
    "    # Directories\n",
    "    repvids_dir: str = \"RepVids\"\n",
    "    repkeys_dir: str = \"RepKeys\"\n",
    "    models_dir: str = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"Pushup Models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PoseAnalyzer:\n",
    "    \"\"\"Handles pose detection and angle calculations.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "       \n",
    "    @staticmethod\n",
    "    def calculate_angle(a: np.ndarray, b: np.ndarray, c: np.ndarray) -> float:\n",
    "        \"\"\"Calculate angle between three points.\"\"\"\n",
    "        radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "        angle = np.abs(radians * 180.0 / np.pi)\n",
    "        return 360 - angle if angle > 180 else angle\n",
    "   \n",
    "    def extract_best_side_landmarks(self, results) -> Optional[Tuple[Dict, str, float]]:\n",
    "        \"\"\"Extract landmarks from the side with better visibility.\"\"\"\n",
    "        if not results.pose_landmarks:\n",
    "            return None\n",
    "           \n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "       \n",
    "        # Define landmark indices for both sides\n",
    "        left_indices = {\n",
    "            'shoulder': self.mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "            'elbow': self.mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            'wrist': self.mp_pose.PoseLandmark.LEFT_WRIST.value,\n",
    "            'hip': self.mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "            'knee': self.mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "            'ankle': self.mp_pose.PoseLandmark.LEFT_ANKLE.value\n",
    "        }\n",
    "       \n",
    "        right_indices = {\n",
    "            'shoulder': self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            'elbow': self.mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            'wrist': self.mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "            'hip': self.mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "            'knee': self.mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            'ankle': self.mp_pose.PoseLandmark.RIGHT_ANKLE.value\n",
    "        }\n",
    "       \n",
    "        # Calculate visibility for both sides\n",
    "        left_vis = np.mean([landmarks[idx].visibility for idx in left_indices.values()])\n",
    "        right_vis = np.mean([landmarks[idx].visibility for idx in right_indices.values()])\n",
    "       \n",
    "        # Choose better side\n",
    "        if left_vis >= right_vis and left_vis > self.config.min_visibility:\n",
    "            indices = left_indices\n",
    "            side = \"Left\"\n",
    "            visibility = left_vis\n",
    "        elif right_vis > self.config.min_visibility:\n",
    "            indices = right_indices\n",
    "            side = \"Right\"\n",
    "            visibility = right_vis\n",
    "        else:\n",
    "            return None\n",
    "           \n",
    "        # Extract coordinates\n",
    "        coords = {}\n",
    "        for joint, idx in indices.items():\n",
    "            coords[joint] = np.array([landmarks[idx].x, landmarks[idx].y])\n",
    "           \n",
    "        return coords, side, visibility\n",
    "   \n",
    "    def get_elbow_angle(self, coords: Dict) -> float:\n",
    "        \"\"\"Calculate elbow angle from coordinates.\"\"\"\n",
    "        return self.calculate_angle(coords['shoulder'], coords['elbow'], coords['wrist'])\n",
    "\n",
    "class VideoProcessor:\n",
    "    \"\"\"Handles video recording and keyframe extraction.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.pose_analyzer = PoseAnalyzer(config)\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "       \n",
    "        # Setup directories\n",
    "        base_path = Path.home() / \"Desktop\" / \"Deep Dive AI Summer 2025\"\n",
    "        self.repvids_path = base_path / config.repvids_dir\n",
    "        self.repkeys_path = base_path / config.repkeys_dir\n",
    "        self.repvids_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.repkeys_path.mkdir(parents=True, exist_ok=True)\n",
    "       \n",
    "    def save_rep_video(self, frames: List[np.ndarray], rep_number: int) -> str:\n",
    "        \"\"\"Save repetition video to RepVids.\"\"\"\n",
    "        if not frames:\n",
    "            return None\n",
    "           \n",
    "        filename = self.repvids_path / f'rep_{rep_number:03d}.mp4'\n",
    "        height, width = frames[0].shape[:2]\n",
    "       \n",
    "        out = cv2.VideoWriter(str(filename), self.fourcc, self.config.fps, (width, height))\n",
    "        for frame in frames:\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "       \n",
    "        print(f\"âœ… Saved rep {rep_number} ({len(frames)} frames)\")\n",
    "        return str(filename)\n",
    "   \n",
    "    def extract_keyframes(self, video_path: str, rep_number: int) -> bool:\n",
    "        \"\"\"Extract and downsample to exactly 30 keyframes based on elbow angles.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "        with self.pose_analyzer.mp_pose.Pose(\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.7\n",
    "        ) as pose:\n",
    "        \n",
    "            frames = []\n",
    "            angles = []\n",
    "        \n",
    "            # Extract all frames and angles\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                frames.append(frame)\n",
    "            \n",
    "                # Get elbow angle\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = pose.process(image_rgb)\n",
    "            \n",
    "                pose_data = self.pose_analyzer.extract_best_side_landmarks(results)\n",
    "                if pose_data:\n",
    "                    coords, _, _ = pose_data\n",
    "                    angle = self.pose_analyzer.get_elbow_angle(coords)\n",
    "                    angles.append(angle)\n",
    "                else:\n",
    "                    angles.append(180)  # Default angle if no pose detected\n",
    "                \n",
    "        cap.release()\n",
    "    \n",
    "        if len(frames) < 3:\n",
    "            return False\n",
    "        \n",
    "        # Find key points\n",
    "        min_angle_idx = np.argmin(angles)\n",
    "    \n",
    "        # Find start and end based on angle peaks\n",
    "        start_idx = 0\n",
    "        end_idx = len(frames) - 1\n",
    "    \n",
    "        # Look for angle peaks before and after minimum\n",
    "        for i in range(min_angle_idx - 1, -1, -1):\n",
    "            if angles[i] > 160:  # High angle indicating \"up\" position\n",
    "                start_idx = i\n",
    "                break\n",
    "            \n",
    "        for i in range(min_angle_idx + 1, len(angles)):\n",
    "            if angles[i] > 160:  # High angle indicating \"up\" position\n",
    "                end_idx = i\n",
    "                break\n",
    "            \n",
    "        # Extract relevant frames\n",
    "        key_frames = frames[start_idx:end_idx + 1]\n",
    "    \n",
    "        # ALWAYS ensure exactly 30 frames\n",
    "        if len(key_frames) == self.config.target_frames:\n",
    "            # Perfect - use as is\n",
    "            final_frames = key_frames\n",
    "        elif len(key_frames) > self.config.target_frames:\n",
    "            # Downsample to exactly 30\n",
    "            indices = np.linspace(0, len(key_frames) - 1, self.config.target_frames, dtype=int)\n",
    "            # Ensure minimum angle frame is included\n",
    "            min_frame_relative = min_angle_idx - start_idx\n",
    "            if min_frame_relative not in indices:\n",
    "                closest_idx = np.argmin(np.abs(indices - min_frame_relative))\n",
    "                indices[closest_idx] = min_frame_relative\n",
    "                indices = np.sort(indices)\n",
    "            final_frames = [key_frames[i] for i in indices]\n",
    "        else:\n",
    "            # Upsample to exactly 30 frames by repeating/interpolating\n",
    "            if len(key_frames) < self.config.target_frames:\n",
    "                # Create indices that will repeat frames to reach target\n",
    "                indices = np.linspace(0, len(key_frames) - 1, self.config.target_frames)\n",
    "                indices = np.round(indices).astype(int)\n",
    "                \n",
    "                # Ensure minimum angle frame is still included\n",
    "                min_frame_relative = min_angle_idx - start_idx\n",
    "                if min_frame_relative < len(key_frames):\n",
    "                    # Find where min frame should be in the upsampled sequence\n",
    "                    target_pos = int((min_frame_relative / (len(key_frames) - 1)) * (self.config.target_frames - 1))\n",
    "                    indices[target_pos] = min_frame_relative\n",
    "                \n",
    "                final_frames = [key_frames[i] for i in indices]\n",
    "    \n",
    "        # Save keyframes video - now guaranteed to be exactly 30 frames\n",
    "        if final_frames:\n",
    "            output_path = self.repkeys_path / f'keyframes_30_rep_{rep_number:03d}.mp4'\n",
    "            height, width = final_frames[0].shape[:2]\n",
    "        \n",
    "            out = cv2.VideoWriter(str(output_path), self.fourcc, 10.0, (width, height))\n",
    "            for frame in final_frames:\n",
    "                out.write(frame)\n",
    "            out.release()\n",
    "        \n",
    "            print(f\"âœ… Extracted {len(final_frames)} keyframes for rep {rep_number}\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "class CNNClassifier:\n",
    "    \"\"\"CNN classifier for processing saved RepKeys videos.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.load_model()\n",
    "        \n",
    "        # Setup RepKeys directory path\n",
    "        base_path = Path.home() / \"Desktop\" / \"Deep Dive AI Summer 2025\"\n",
    "        self.repkeys_path = base_path / config.repkeys_dir\n",
    "       \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the latest CNN model.\"\"\"\n",
    "        os.makedirs(self.config.models_dir, exist_ok=True)\n",
    "        model_files = glob.glob(os.path.join(self.config.models_dir, 'pushup_classification*.h5'))\n",
    "       \n",
    "        if model_files:\n",
    "            latest_model = max(model_files, key=os.path.getmtime)\n",
    "            self.model = load_model(latest_model)\n",
    "            print(f\"âœ… Loaded CNN model: {os.path.basename(latest_model)}\")\n",
    "        else:\n",
    "            print(\"âŒ No CNN model found - classification disabled\")\n",
    "            self.model = None\n",
    "           \n",
    "    def preprocess_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess frame for CNN.\"\"\"\n",
    "        processed = cv2.resize(frame, self.config.frame_size)\n",
    "        return (processed / 255.0).astype('float32')\n",
    "    \n",
    "    def load_video_frames(self, video_path: str) -> Optional[List[np.ndarray]]:\n",
    "        \"\"\"Load all frames from a video file.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "            \n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) != self.config.target_frames:\n",
    "            print(f\"âš ï¸  Video {os.path.basename(video_path)} has {len(frames)} frames, expected {self.config.target_frames}\")\n",
    "            \n",
    "        return frames if frames else None\n",
    "    \n",
    "    def predict_rep_classification(self, video_path: str, rep_number: int) -> Tuple[str, float]:\n",
    "        \"\"\"Predict classification for a single rep video.\"\"\"\n",
    "        if self.model is None:\n",
    "            return \"No Model Available\", 0.0\n",
    "            \n",
    "        try:\n",
    "            # Load video frames\n",
    "            frames = self.load_video_frames(video_path)\n",
    "            if not frames:\n",
    "                return \"Error: No frames loaded\", 0.0\n",
    "            \n",
    "            # Preprocess frames\n",
    "            processed_frames = [self.preprocess_frame(frame) for frame in frames]\n",
    "            \n",
    "            # Pad or truncate to target_frames if necessary\n",
    "            if len(processed_frames) < self.config.target_frames:\n",
    "                # Repeat last frame to reach target\n",
    "                while len(processed_frames) < self.config.target_frames:\n",
    "                    processed_frames.append(processed_frames[-1])\n",
    "            elif len(processed_frames) > self.config.target_frames:\n",
    "                # Truncate to target frames\n",
    "                processed_frames = processed_frames[:self.config.target_frames]\n",
    "            \n",
    "            # Prepare sequence for model\n",
    "            sequence = np.array(processed_frames, dtype='float32')\n",
    "            sequence = np.expand_dims(sequence, axis=0)  # Add batch dimension\n",
    "            \n",
    "            # Make prediction\n",
    "            prediction = self.model.predict(sequence, verbose=0)\n",
    "            predicted_class = np.argmax(prediction)\n",
    "            confidence = np.max(prediction) * 100\n",
    "            \n",
    "            labels = ['Incorrect Form', 'Correct Form']\n",
    "            classification = labels[predicted_class]\n",
    "            \n",
    "            # Print result\n",
    "            print(f\"ðŸ” Rep {rep_number}: {classification} (Confidence: {confidence:.1f}%)\")\n",
    "            \n",
    "            return classification, confidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing rep {rep_number}: {e}\"\n",
    "            print(f\"âŒ {error_msg}\")\n",
    "            return \"Error\", 0.0\n",
    "    \n",
    "    def process_all_repkeys(self):\n",
    "        \"\"\"Process all videos in RepKeys directory and print classifications.\"\"\"\n",
    "        if not self.repkeys_path.exists():\n",
    "            print(f\"âŒ RepKeys directory not found: {self.repkeys_path}\")\n",
    "            return\n",
    "            \n",
    "        # Find all RepKeys videos\n",
    "        video_files = list(self.repkeys_path.glob(\"keyframes_30_rep_*.mp4\"))\n",
    "        \n",
    "        if not video_files:\n",
    "            print(\"âŒ No RepKeys videos found for classification\")\n",
    "            return\n",
    "            \n",
    "        # Sort by rep number\n",
    "        video_files.sort(key=lambda x: int(x.stem.split('_')[-1]))\n",
    "        \n",
    "        print(f\"\\nðŸ¤– CNN Classification Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for video_file in video_files:\n",
    "            # Extract rep number from filename\n",
    "            try:\n",
    "                rep_number = int(video_file.stem.split('_')[-1])\n",
    "                self.predict_rep_classification(str(video_file), rep_number)\n",
    "            except ValueError:\n",
    "                print(f\"âš ï¸  Could not extract rep number from {video_file.name}\")\n",
    "                \n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    def process_single_rep(self, rep_number: int) -> Tuple[str, float]:\n",
    "        \"\"\"Process a single rep by number.\"\"\"\n",
    "        video_path = self.repkeys_path / f\"keyframes_30_rep_{rep_number:03d}.mp4\"\n",
    "        \n",
    "        if not video_path.exists():\n",
    "            print(f\"âŒ Rep {rep_number} video not found: {video_path}\")\n",
    "            return \"File Not Found\", 0.0\n",
    "            \n",
    "        return self.predict_rep_classification(str(video_path), rep_number)\n",
    "\n",
    "class EfficientPushupDetector:\n",
    "    \"\"\"Main efficient pushup detection system.\"\"\"\n",
    "   \n",
    "    def __init__(self, config: Config = None):\n",
    "        self.config = config or Config()\n",
    "       \n",
    "        # Initialize components\n",
    "        self.pose_analyzer = PoseAnalyzer(self.config)\n",
    "        self.video_processor = VideoProcessor(self.config)\n",
    "        self.cnn_classifier = CNNClassifier(self.config)\n",
    "       \n",
    "        # MediaPipe setup\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "       \n",
    "        # Detection state\n",
    "        self.counter = 0\n",
    "        self.stage = None\n",
    "        self.frame_buffer = []\n",
    "        self.first_rep_buffer = []\n",
    "        self.first_stage_up_detected = False\n",
    "        self.is_first_rep = True\n",
    "        self.processing_executor = concurrent.futures.ThreadPoolExecutor(max_workers=3)\n",
    "       \n",
    "    def process_completed_rep(self, frames: List[np.ndarray], rep_number: int):\n",
    "        \"\"\"Process completed rep in background thread.\"\"\"\n",
    "        def process():\n",
    "            # Save to RepVids\n",
    "            video_path = self.video_processor.save_rep_video(frames, rep_number)\n",
    "            if video_path:\n",
    "                # Extract keyframes to RepKeys\n",
    "                success = self.video_processor.extract_keyframes(video_path, rep_number)\n",
    "                \n",
    "                # If keyframes were successfully extracted, classify the rep\n",
    "                if success:\n",
    "                    # Add small delay to ensure file is fully written\n",
    "                    time.sleep(0.5)\n",
    "                    self.cnn_classifier.process_single_rep(rep_number)\n",
    "               \n",
    "        self.processing_executor.submit(process)\n",
    "       \n",
    "    def run(self, camera_index: int = 0):\n",
    "        \"\"\"Run the efficient detection system.\"\"\"\n",
    "        cap = cv2.VideoCapture(camera_index)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "       \n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open camera {camera_index}\")\n",
    "            return\n",
    "           \n",
    "        print(\"ðŸš€ Efficient Pushup Detection System\")\n",
    "        print(\"ðŸ“¹ Live detection + ðŸ”„ Auto-processing + ðŸ¤– CNN Classification\")\n",
    "        print(\"Press 'q' to quit\")\n",
    "       \n",
    "        try:\n",
    "            with self.mp_pose.Pose(\n",
    "                min_detection_confidence=self.config.min_detection_confidence,\n",
    "                min_tracking_confidence=self.config.min_tracking_confidence\n",
    "            ) as pose:\n",
    "               \n",
    "                while cap.isOpened():\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                       \n",
    "                    # Process pose\n",
    "                    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    results = pose.process(image_rgb)\n",
    "                   \n",
    "                    # Extract pose data\n",
    "                    pose_data = self.pose_analyzer.extract_best_side_landmarks(results)\n",
    "                   \n",
    "                    if pose_data:\n",
    "                        coords, side, visibility = pose_data\n",
    "                        elbow_angle = self.pose_analyzer.get_elbow_angle(coords)\n",
    "                       \n",
    "                        # Update detection state\n",
    "                        prev_counter = self.counter\n",
    "                       \n",
    "                        if elbow_angle > self.config.elbow_angle_up:\n",
    "                            if self.stage == \"down\":\n",
    "                                self.stage = \"up\"\n",
    "                                self.counter += 1\n",
    "                            else:\n",
    "                                self.stage = \"up\"\n",
    "                        elif elbow_angle < self.config.elbow_angle_down and self.stage == \"up\":\n",
    "                            self.stage = \"down\"\n",
    "                           \n",
    "                        # Handle frame buffering - always add to normal buffer now\n",
    "                        if not self.is_first_rep:\n",
    "                            self.frame_buffer.append(frame.copy())\n",
    "\n",
    "                        if self.is_first_rep:\n",
    "                            # Start collecting frames when stage first becomes \"up\"\n",
    "                            if self.stage == \"up\" and not self.first_stage_up_detected:\n",
    "                                self.first_stage_up_detected = True\n",
    "                                self.first_rep_buffer = [frame.copy()]\n",
    "                                print(\"ðŸ”´ First rep: Started collecting frames (stage = 'up')\")\n",
    "                            \n",
    "                            # Continue collecting frames if we've started\n",
    "                            elif self.first_stage_up_detected:\n",
    "                                self.first_rep_buffer.append(frame.copy())\n",
    "                            \n",
    "                            # Save first rep when counter increases from 0 to 1\n",
    "                            if self.counter > prev_counter and self.counter == 1:\n",
    "                                # Rep completed - process in background\n",
    "                                self.process_completed_rep(self.first_rep_buffer.copy(), self.counter)\n",
    "                                \n",
    "                                # Reset first rep variables\n",
    "                                self.first_rep_buffer = []\n",
    "                                self.is_first_rep = False\n",
    "                                print(\"ðŸ”„ Switching to normal rep recording mode\")\n",
    "\n",
    "                        elif self.counter > prev_counter:\n",
    "                            # Rep completed - process in background\n",
    "                            self.process_completed_rep(self.frame_buffer.copy(), self.counter)\n",
    "                            self.frame_buffer = []\n",
    "                            \n",
    "                        # Display angle info\n",
    "                        percent = np.interp(elbow_angle, [90, 160], [0, 100])\n",
    "                        cv2.putText(frame, f'{side} Elbow: {int(elbow_angle)}*', (10, 60),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                        cv2.putText(frame, f'{int(percent)}%', (10, 90),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "                               \n",
    "                    # Draw pose landmarks\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=3),\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "                    )\n",
    "                   \n",
    "                    # Rep counter\n",
    "                    cv2.rectangle(frame, (frame.shape[1] - 120, 0), (frame.shape[1], 60), (0, 0, 0), -1)\n",
    "                    cv2.putText(frame, 'REPS', (frame.shape[1] - 110, 25),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                    cv2.putText(frame, str(self.counter), (frame.shape[1] - 100, 50),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 3)\n",
    "                               \n",
    "                    # Buffer status\n",
    "                    if self.is_first_rep:\n",
    "                        if self.first_stage_up_detected:\n",
    "                            cv2.putText(frame, f'First Rep Buffer: {len(self.first_rep_buffer)}', \n",
    "                                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "                        else:\n",
    "                            cv2.putText(frame, 'Waiting for first \"up\" stage...', \n",
    "                                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "                    else:\n",
    "                        cv2.putText(frame, f'Buffer: {len(self.frame_buffer)}', (10, 30),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "                               \n",
    "                    cv2.imshow('Efficient Pushup Detection', frame)\n",
    "                   \n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                       \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            self.processing_executor.shutdown(wait=False)\n",
    "            \n",
    "        # Process any remaining RepKeys videos when exiting\n",
    "        print(\"\\nðŸ” Final classification of all reps:\")\n",
    "        self.cnn_classifier.process_all_repkeys()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the efficient pushup detection system.\"\"\"\n",
    "    config = Config(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "        elbow_angle_up=165.0,\n",
    "        elbow_angle_down=95.0,\n",
    "        target_frames=30\n",
    "    )\n",
    "   \n",
    "    detector = EfficientPushupDetector(config)\n",
    "    detector.run()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# CLEAR REPVIDS AND REPKEYS FILES BEFORE RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4198d",
   "metadata": {},
   "source": [
    "# why is it labelling incorrect with 100% confidence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af40c60",
   "metadata": {},
   "source": [
    "Looking at your code, there are several potential reasons why your CNN model is predicting everything as \"Incorrect\" with 100% confidence:\n",
    "\n",
    "### **Most Likely Issues:**\n",
    "\n",
    "#### 1. *Model-Data Mismatch*\n",
    "Your model was likely trained on data with a different preprocessing pipeline or frame ordering than what you're using in the live system.\n",
    "\n",
    "#### 2. *Frame Preprocessing Differences*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ca49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Preprocess frame for CNN.\"\"\"\n",
    "    processed = cv2.resize(frame, self.config.frame_size)  # (112, 112)\n",
    "    return (processed / 255.0).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baada15f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Check if your training data was preprocessed differently:**\n",
    "- Different normalization (e.g., mean subtraction, different scaling)\n",
    "- Different color space (RGB vs BGR)\n",
    "- Different frame ordering or timing\n",
    "\n",
    "#### 3. *Sequence Length Mismatch*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad or truncate to target_frames if necessary\n",
    "if len(processed_frames) < self.config.target_frames:\n",
    "    # Repeat last frame to reach target\n",
    "    while len(processed_frames) < self.config.target_frames:\n",
    "        processed_frames.append(processed_frames[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607d943",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**This padding with repeated frames is problematic** - it creates artificial sequences that likely don't match your training data distribution.\n",
    "\n",
    "#### 4. *Frame Quality/Content Issues*\n",
    "Your keyframe extraction might be producing sequences that are fundamentally different from your training data in terms of:\n",
    "- Frame transitions\n",
    "- Pose angles captured\n",
    "- Movement patterns\n",
    "\n",
    "### **Quick Diagnostic Steps:**\n",
    "\n",
    "#### 1. *Test with Known Good Data*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd0bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this method to your CNNClassifier class\n",
    "def test_with_training_data(self):\n",
    "    \"\"\"Test model with a sample from your original training dataset\"\"\"\n",
    "    # Load a video you KNOW should be classified correctly\n",
    "    # from your original training data and test it\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7747a0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. *Check Model Labels*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your predict_rep_classification method, add debugging:\n",
    "def predict_rep_classification(self, video_path: str, rep_number: int) -> Tuple[str, float]:\n",
    "    # ... existing code ...\n",
    "    \n",
    "    # Add debugging\n",
    "    print(f\"Prediction raw output: {prediction}\")\n",
    "    print(f\"Prediction shape: {prediction.shape}\")\n",
    "    print(f\"Predicted class index: {predicted_class}\")\n",
    "    print(f\"Class probabilities: {prediction[0]}\")\n",
    "    \n",
    "    # ... rest of method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb481e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3. *Verify Frame Sequence*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a842397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this debugging in extract_keyframes:\n",
    "def extract_keyframes(self, video_path: str, rep_number: int) -> bool:\n",
    "    # ... existing code ...\n",
    "    \n",
    "    # Debug the final frames\n",
    "    print(f\"Final frames count: {len(final_frames)}\")\n",
    "    print(f\"Frame dimensions: {final_frames[0].shape if final_frames else 'None'}\")\n",
    "    \n",
    "    # Save a sample frame to visually inspect\n",
    "    if final_frames:\n",
    "        cv2.imwrite(f\"debug_frame_rep_{rep_number}.jpg\", final_frames[len(final_frames)//2])\n",
    "    \n",
    "    # ... rest of method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b568f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Most Likely Fix:**\n",
    "\n",
    "The issue is probably in the **frame padding/truncation logic**. Instead of repeating the last frame, try:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rep_classification(self, video_path: str, rep_number: int) -> Tuple[str, float]:\n",
    "    if self.model is None:\n",
    "        return \"No Model Available\", 0.0\n",
    "        \n",
    "    try:\n",
    "        frames = self.load_video_frames(video_path)\n",
    "        if not frames:\n",
    "            return \"Error: No frames loaded\", 0.0\n",
    "        \n",
    "        # BETTER HANDLING: Only process if we have reasonable frame count\n",
    "        if len(frames) < 20:  # Too few frames\n",
    "            return \"Insufficient Frames\", 0.0\n",
    "            \n",
    "        # If too many frames, use intelligent subsampling instead of truncation\n",
    "        if len(frames) > self.config.target_frames:\n",
    "            indices = np.linspace(0, len(frames)-1, self.config.target_frames, dtype=int)\n",
    "            frames = [frames[i] for i in indices]\n",
    "        elif len(frames) < self.config.target_frames:\n",
    "            # Instead of padding, return error or use what we have\n",
    "            return \"Frame Count Mismatch\", 0.0\n",
    "            \n",
    "        # ... rest of preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e6147",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Check your training data preprocessing and ensure it matches exactly what you're doing in the live system!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216c343",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8666f5da",
   "metadata": {},
   "source": [
    "Notes to fix:\n",
    "\n",
    "make Rep Counter pretty\n",
    "\n",
    "add rolling window\n",
    "\n",
    "add output folder for labelled stuff?\n",
    "\n",
    "train model more?\n",
    "\n",
    "no degree sign for cv.putText(not even \\u00B0) - replaced with *\n",
    "\n",
    "check accuracy of model\n",
    " - WTF WHY IS IT NOT WORKING\n",
    " - says everything is 100% incorrect for sm reason"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
